{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5160c52e",
   "metadata": {},
   "source": [
    "# Novel Extension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f768a8b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3f768a8b",
    "outputId": "6c021311-ec12-4dca-a1fa-17972685b616"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "import requests\n",
    "import tarfile\n",
    "import math\n",
    "from PIL import Image\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "import torchvision.models as models\n",
    "from torchvision import transforms\n",
    "from typing import Callable, Tuple\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "def set_seed(seed: int) -> None:\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "url = \"https://s3.amazonaws.com/fast-ai-imagelocal/biwi_head_pose.tgz\"\n",
    "save_path = \"biwi_head_pose.tgz\"\n",
    "data_dir = \"biwi_head_pose\"\n",
    "\n",
    "if not os.path.exists(data_dir):\n",
    "    print(\"Downloading BIWI dataset...\")\n",
    "    response = requests.get(url, stream=True)\n",
    "    with open(save_path, \"wb\") as f:\n",
    "        for chunk in response.iter_content(chunk_size=8192):\n",
    "            f.write(chunk)\n",
    "\n",
    "    print(\"Extracting...\")\n",
    "    with tarfile.open(save_path, \"r:gz\") as tar:\n",
    "        tar.extractall()\n",
    "    print(\"Done!\")\n",
    "else:\n",
    "    print(\"Dataset already exists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3b3863",
   "metadata": {
    "id": "8d3b3863"
   },
   "outputs": [],
   "source": [
    "class BiwiDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.image_paths = sorted(glob(f\"{root_dir}/*/*_rgb.jpg\"))  # RGB only\n",
    "\n",
    "    def select_faces(self,face_ids):\n",
    "        self.image_paths = []\n",
    "        for face_id in face_ids:\n",
    "            self.image_paths += sorted(glob(f\"{self.root_dir}/{face_id:0>2}/*_rgb.jpg\"))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def convert_matrix_to_euler(self, rotation_matrix):\n",
    "        R = rotation_matrix\n",
    "        sy = np.sqrt(R[0, 0] * R[0, 0] + R[1, 0] * R[1, 0])\n",
    "        singular = sy < 1e-6\n",
    "\n",
    "        if not singular:\n",
    "            x = np.arctan2(R[2, 1], R[2, 2])\n",
    "            y = np.arctan2(-R[2, 0], sy)\n",
    "            z = np.arctan2(R[1, 0], R[0, 0])\n",
    "        else:\n",
    "            x = np.arctan2(-R[1, 2], R[1, 1])\n",
    "            y = np.arctan2(-R[2, 0], sy)\n",
    "            z = 0\n",
    "        return np.array([x, y, z]) * (180 / np.pi)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        label_path = img_path.replace(\"_rgb.jpg\", \"_pose.txt\")\n",
    "        with open(label_path, \"r\") as f:\n",
    "            lines = f.readlines()\n",
    "            matrix = np.array(\n",
    "                [\n",
    "                    [float(v) for v in lines[0].strip().split()],\n",
    "                    [float(v) for v in lines[1].strip().split()],\n",
    "                    [float(v) for v in lines[2].strip().split()],\n",
    "                ]\n",
    "            )\n",
    "\n",
    "        # Convert Matrix to Angles - (Pitch, Yaw, Roll)\n",
    "        angles = self.convert_matrix_to_euler(matrix).astype(np.float32)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, torch.tensor(angles)\n",
    "\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "pool_dataset = BiwiDataset(data_dir, transform=transform)\n",
    "pool_dataset.select_faces(list(range(1,21)))\n",
    "test_dataset = BiwiDataset(data_dir, transform=transform)\n",
    "test_dataset.select_faces(list(range(1,25)))\n",
    "\n",
    "def make_loader(dataset,indices, batch_size, shuffle):\n",
    "    subset = Subset(dataset, indices.tolist())\n",
    "    return DataLoader(subset, batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "def prep_data(seed, pool_ratio, pool_size, val_size, test_size):\n",
    "    \"\"\"\n",
    "    Prepare data for the experiments.\n",
    "    Returns:\n",
    "        loaders : Tuple[loader] init, pool, deterministic, val, test loaders\n",
    "        pool_dataset : full dataset of available points\n",
    "        idxs: Tuple[torch.Tensor] labeled, pool indices - to keep consistent starting splits across runs\n",
    "    \"\"\"\n",
    "    total = len(pool_dataset)\n",
    "    if total < pool_size + val_size:\n",
    "        raise ValueError(\"Not enough points available in dataset\")\n",
    "    init_size = int(pool_ratio * pool_size)\n",
    "    # Make splits\n",
    "    perm = torch.randperm(total, generator=torch.Generator().manual_seed(seed))\n",
    "    idx_labeled = perm[:init_size]\n",
    "    idx_pool = perm[init_size :pool_size]\n",
    "    idx_val = perm[pool_size : pool_size + val_size]\n",
    "    test_perm = torch.randperm(len(test_dataset), generator=torch.Generator().manual_seed(seed))\n",
    "    idx_test = test_perm[:test_size]\n",
    "    # Make loaders\n",
    "    init_loader = make_loader(pool_dataset,idx_labeled, batch_size=32, shuffle=True)\n",
    "    pool_loader = make_loader(pool_dataset,idx_pool, batch_size=64, shuffle=False)\n",
    "    determ_train_loader = make_loader(pool_dataset, torch.cat((idx_labeled,idx_pool)), batch_size=128, shuffle=False)\n",
    "    val_loader = make_loader(pool_dataset,idx_val, batch_size=64, shuffle=False)\n",
    "    test_loader = make_loader(test_dataset,idx_test, batch_size=64, shuffle=True)\n",
    "    loaders = (init_loader, pool_loader, determ_train_loader, val_loader, test_loader)\n",
    "    idxs = (idx_labeled, idx_pool)\n",
    "    return loaders, pool_dataset, idxs\n",
    "\n",
    "\n",
    "def compute_label_stats(loader: DataLoader, device: torch.device) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"Compute mean and std of labels from a loader.\"\"\"\n",
    "    ys = []\n",
    "    with torch.no_grad():\n",
    "        for _, batch_y in loader:\n",
    "            ys.append(batch_y)\n",
    "    y_all = torch.cat(ys, dim=0)\n",
    "    y_mean = y_all.mean(dim=0)\n",
    "    y_std = y_all.std(dim=0) + 1e-6\n",
    "    return y_mean.to(device), y_std.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff779a1",
   "metadata": {
    "id": "0ff779a1"
   },
   "outputs": [],
   "source": [
    "class EfficientNetBasis(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Load the full model\n",
    "        weights = models.EfficientNet_B0_Weights.DEFAULT\n",
    "        full_model = models.efficientnet_b0(weights=weights)\n",
    "        # EfficientNet has .features (convolutions) and .classifier (linear)\n",
    "        self.features = full_model.features\n",
    "        self.pool = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "        # Freeze parameters\n",
    "        for param in self.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.pool(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cace347",
   "metadata": {
    "id": "6cace347"
   },
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "class BayesianRegressionHead(ABC):\n",
    "    \"\"\"Base class for Bayesian linear regression heads.\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        basis_dim: int,\n",
    "        num_outputs: int,\n",
    "        likelihood_variance: float = 1.0,\n",
    "        prior_variance: float = 1.0,\n",
    "        jitter: float = 1e-6,\n",
    "    ):\n",
    "        self.basis_dim = basis_dim\n",
    "        self.num_outputs = num_outputs\n",
    "        self.likelihood_variance = likelihood_variance\n",
    "        self.prior_variance = prior_variance\n",
    "        self.jitter = jitter\n",
    "        self.posterior_mean: Optional[torch.Tensor] = None  # shape (basis_dim, num_outputs)\n",
    "        self.posterior_cov: Optional[torch.Tensor] = None   # shape (basis_dim, basis_dim)\n",
    "\n",
    "    @abstractmethod\n",
    "    def forward(self, phi: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Predict given basis functions phi: (batch, basis_dim) -> (batch, num_outputs)\"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def predict_with_uncertainty(self, phi: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Return (mean, variance) predictions.\"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def compute_posterior(self, phi: torch.Tensor, y: torch.Tensor) -> None:\n",
    "        \"\"\"Fit posterior given basis functions and targets.\"\"\"\n",
    "        pass\n",
    "\n",
    "\n",
    "class AnalyticalBayesianHead(BayesianRegressionHead):\n",
    "    \"\"\"Closed-form analytical Bayesian linear regression.\"\"\"\n",
    "\n",
    "    def forward(self, phi: torch.Tensor) -> torch.Tensor:\n",
    "        if self.posterior_mean is None:\n",
    "            return phi  # Return raw basis if no posterior\n",
    "        return phi @ self.posterior_mean\n",
    "\n",
    "    def predict_with_uncertainty(\n",
    "        self, phi: torch.Tensor, include_likelihood_noise: bool = True\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        if self.posterior_mean is None or self.posterior_cov is None:\n",
    "            base_var = torch.zeros(phi.size(0), device=phi.device, dtype=phi.dtype)\n",
    "            if include_likelihood_noise:\n",
    "                base_var = base_var + self.likelihood_variance\n",
    "            pred_var = base_var.unsqueeze(1).expand(-1, self.num_outputs)\n",
    "            return phi, pred_var\n",
    "\n",
    "        pred_mean = phi @ self.posterior_mean\n",
    "        phi_S = phi @ self.posterior_cov\n",
    "        epistemic = torch.sum(phi_S * phi, dim=1)\n",
    "        total_var = epistemic + self.likelihood_variance if include_likelihood_noise else epistemic\n",
    "        pred_var = total_var.unsqueeze(1).expand(-1, self.num_outputs)\n",
    "        return pred_mean, pred_var\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def compute_posterior(self, phi: torch.Tensor, y: torch.Tensor) -> None:\n",
    "        sigma_inv = 1.0 / self.likelihood_variance\n",
    "        prior_inv = 1.0 / self.prior_variance\n",
    "\n",
    "        phiT_phi = phi.T @ phi\n",
    "        eye = torch.eye(phi.shape[1], device=phi.device, dtype=phi.dtype)\n",
    "        s_inv = sigma_inv * phiT_phi + prior_inv * eye\n",
    "        s_inv = s_inv + self.jitter * eye\n",
    "        s_cov = torch.linalg.inv(s_inv)\n",
    "\n",
    "        phiT_y = phi.T @ y\n",
    "        mu = s_cov @ (sigma_inv * phiT_y)\n",
    "\n",
    "        self.posterior_cov = s_cov\n",
    "        self.posterior_mean = mu\n",
    "\n",
    "\n",
    "class MFVIBayesianHead(BayesianRegressionHead):\n",
    "    \"\"\"Mean-field variational inference Bayesian linear regression (closed-form updates).\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        basis_dim: int,\n",
    "        num_outputs: int,\n",
    "        likelihood_variance: float = 1.0,\n",
    "        prior_variance: float = 1.0,\n",
    "        jitter: float = 1e-6,\n",
    "    ):\n",
    "        super().__init__(basis_dim, num_outputs, likelihood_variance, prior_variance, jitter)\n",
    "\n",
    "    def forward(self, phi: torch.Tensor) -> torch.Tensor:\n",
    "        if self.posterior_mean is None:\n",
    "            return phi\n",
    "        return phi @ self.posterior_mean\n",
    "\n",
    "    def predict_with_uncertainty(self, phi: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        if self.posterior_mean is None or self.posterior_cov is None:\n",
    "            base_var = torch.zeros(phi.size(0), device=phi.device, dtype=phi.dtype)\n",
    "            base_var = base_var + self.likelihood_variance\n",
    "            pred_var = base_var.unsqueeze(1).expand(-1, self.num_outputs)\n",
    "            return phi, pred_var\n",
    "        predictive_mean = phi @ self.posterior_mean\n",
    "        phi_S = phi @ self.posterior_cov\n",
    "        epistemic = torch.sum(phi_S * phi, dim=1)\n",
    "        total_var = epistemic + self.likelihood_variance\n",
    "        pred_var = total_var.unsqueeze(1).expand(-1, self.num_outputs)\n",
    "\n",
    "        return predictive_mean, pred_var\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def compute_posterior(self, phi: torch.Tensor, y: torch.Tensor) -> None:\n",
    "        sigma2 = self.likelihood_variance\n",
    "        prior_var = self.prior_variance\n",
    "        D = phi.shape[1]\n",
    "\n",
    "        phiT_phi = phi.T @ phi\n",
    "        eye = torch.eye(D, device=phi.device, dtype=phi.dtype)\n",
    "\n",
    "        s_inv = (1.0 / sigma2) * phiT_phi + (1.0 / prior_var) * eye\n",
    "        s_inv = s_inv + self.jitter * eye\n",
    "        s_cov = torch.linalg.inv(s_inv)\n",
    "\n",
    "        phiT_y = phi.T @ y\n",
    "        mu = s_cov @ ((1.0 / sigma2) * phiT_y)\n",
    "\n",
    "        diag_vals = torch.diag(s_cov)\n",
    "        self.posterior_cov = torch.diag(diag_vals).detach()\n",
    "        self.posterior_mean = mu.detach()\n",
    "\n",
    "class LaplaceMFVIBayesianHead(BayesianRegressionHead):\n",
    "    \"\"\"Mean-field variational inference Bayesian linear regression with Laplace Prior\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        basis_dim: int,\n",
    "        num_outputs: int,\n",
    "        prior_variance: float = 1.0,\n",
    "        jitter: float = 1e-6,\n",
    "        likelihood_scale: float = 1.0,\n",
    "        learning_rate: float = 1e-3,\n",
    "        num_iters: int = 1000,\n",
    "        batch_size: int = 64,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(basis_dim, num_outputs, 2*(likelihood_scale**2), prior_variance, jitter)\n",
    "        self.likelihood_scale = likelihood_scale\n",
    "        self.lr = learning_rate\n",
    "        self.steps = num_iters\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "\n",
    "    def forward(self, phi: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Predictive mean using posterior mean weights.\"\"\"\n",
    "        if self.posterior_mean is None:\n",
    "            return phi @ self.q_mu\n",
    "        return phi @ self.posterior_mean\n",
    "\n",
    "    def predict_with_uncertainty(self, phi: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Predictive mean and variance.\n",
    "        \"\"\"\n",
    "        if self.posterior_mean is None or self.posterior_cov is None:\n",
    "            base_var = torch.zeros(phi.size(0), device=phi.device, dtype=phi.dtype)\n",
    "            base_var = base_var + self.likelihood_variance\n",
    "            pred_var = base_var.unsqueeze(1).expand(-1, self.num_outputs)\n",
    "            return phi, pred_var\n",
    "        predictive_mean = phi @ self.posterior_mean\n",
    "        var_w = self.posterior_cov\n",
    "        phi_sq = phi ** 2\n",
    "        epistemic_var = phi_sq @ var_w\n",
    "        aleatoric_var = 2 * (self.likelihood_scale ** 2)\n",
    "        predictive_var = epistemic_var + aleatoric_var\n",
    "\n",
    "        return predictive_mean, predictive_var\n",
    "\n",
    "    def _kl_divergence(self, mu: torch.Tensor, log_var: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Compute KL(q(w) || p(w)) where both are Gaussian.\n",
    "        p(w) ~ N(0, prior_variance * I)\n",
    "        q(w) ~ N(mu, exp(log_var) * I)\n",
    "        \"\"\"\n",
    "        var = torch.exp(log_var)\n",
    "        prior_var = self.prior_variance\n",
    "        kl = 0.5 * torch.sum(\n",
    "            (var / prior_var) +\n",
    "            (mu**2 / prior_var) -\n",
    "            1.0 -\n",
    "            (log_var - math.log(prior_var))\n",
    "        )\n",
    "        return kl\n",
    "\n",
    "    def compute_posterior(self, phi: torch.Tensor, y: torch.Tensor) -> None:\n",
    "        \"\"\"\n",
    "        Fit posterior using Gradient Descent (Adam) instead of Coordinate Ascent.\n",
    "        We optimize local tensors and then save the result to self.posterior_mean.\n",
    "        \"\"\"\n",
    "        device = phi.device\n",
    "        N, D = phi.shape\n",
    "        num_outputs = y.shape[1]\n",
    "        q_mu = torch.zeros(D, num_outputs, device=device, requires_grad=True)\n",
    "        with torch.no_grad():\n",
    "             q_mu.normal_(0, 0.01)\n",
    "\n",
    "        q_log_var = torch.ones(D, num_outputs, device=device, requires_grad=True)\n",
    "        with torch.no_grad():\n",
    "             init_log_var = math.log(self.prior_variance)\n",
    "             q_log_var.fill_(init_log_var)\n",
    "        optimizer = torch.optim.Adam([q_mu, q_log_var], lr=self.lr)\n",
    "        dataset = torch.utils.data.TensorDataset(phi, y)\n",
    "        loader = torch.utils.data.DataLoader(dataset, batch_size=self.batch_size, shuffle=True)\n",
    "        for step in range(self.steps):\n",
    "            for batch_phi, batch_y in loader:\n",
    "                optimizer.zero_grad()\n",
    "                q_log_var_clamped = torch.clamp(q_log_var, min=-10, max=2)\n",
    "                std = torch.exp(0.5 * q_log_var_clamped)\n",
    "                epsilon = torch.randn_like(std)\n",
    "                w_sample = q_mu + std * epsilon\n",
    "                y_pred = batch_phi @ w_sample\n",
    "                l1_loss = torch.mean(torch.abs(batch_y - y_pred))\n",
    "                nll = l1_loss / self.likelihood_scale\n",
    "                kl = self._kl_divergence(q_mu, q_log_var_clamped)\n",
    "                loss = nll + kl/N\n",
    "\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_([q_mu, q_log_var], max_norm=1.0)\n",
    "                optimizer.step()\n",
    "        self.posterior_mean = q_mu.detach()\n",
    "        self.posterior_cov = torch.exp(q_log_var).detach()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def update_likelihood_scale(self, mae_value: float) -> None:\n",
    "        \"\"\"\n",
    "        Updates the internal likelihood scale (b) based on an externally\n",
    "        computed Mean Absolute Error.\n",
    "        \"\"\"\n",
    "        new_scale = max(float(mae_value), 1e-6)\n",
    "        self.likelihood_scale = new_scale\n",
    "        self.likelihood_scale = min(self.likelihood_scale, 2.0)\n",
    "        self.likelihood_variance = 2 * (new_scale ** 2)\n",
    "\n",
    "\n",
    "class BayesianEfficientNetModel(nn.Module):\n",
    "    \"\"\"Combines the EfficientNet backbone + Bayesian linear head.\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        backbone: nn.Module,\n",
    "        num_outputs: int = 3,\n",
    "        head_type: str = \"analytical\",\n",
    "        likelihood_variance: float = 1.0,\n",
    "        prior_variance: float = 1.0,\n",
    "        likelihood_scale: float = 1.0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "        self.num_outputs = num_outputs\n",
    "        self.likelihood_variance = likelihood_variance\n",
    "        self.head_type = head_type\n",
    "        with torch.no_grad():\n",
    "            dummy_input = torch.randn(1, 3, 224, 224)\n",
    "            basis = backbone(dummy_input)\n",
    "            basis_dim = basis.shape[1]\n",
    "        if head_type == \"analytical\":\n",
    "            self.head = AnalyticalBayesianHead(\n",
    "                basis_dim=basis_dim,\n",
    "                num_outputs=num_outputs,\n",
    "                likelihood_variance=likelihood_variance,\n",
    "                prior_variance=prior_variance,\n",
    "            )\n",
    "        elif head_type == \"mfvi\":\n",
    "            self.head = MFVIBayesianHead(\n",
    "                basis_dim=basis_dim,\n",
    "                num_outputs=num_outputs,\n",
    "                likelihood_variance=likelihood_variance,\n",
    "                prior_variance=prior_variance,\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown head_type: {head_type}\")\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        phi = self.backbone(x)\n",
    "        return self.head.forward(phi)\n",
    "\n",
    "    def predict_with_uncertainty(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        phi = self.backbone(x)\n",
    "        return self.head.predict_with_uncertainty(phi)\n",
    "\n",
    "    def fit_posterior(self, loader: torch.utils.data.DataLoader, device: torch.device, y_mean: Optional[torch.Tensor] = None, y_std: Optional[torch.Tensor] = None) -> None:\n",
    "        self.backbone.eval()\n",
    "        with torch.no_grad():\n",
    "            phi_list = []\n",
    "            y_list = []\n",
    "            for batch_x, batch_y in loader:\n",
    "                batch_x = batch_x.to(device)\n",
    "                batch_y = batch_y.to(device)\n",
    "                phi = self.backbone(batch_x)\n",
    "                phi_list.append(phi)\n",
    "                y_list.append(batch_y)\n",
    "\n",
    "            phi_all = torch.cat(phi_list, dim=0)\n",
    "            y_all = torch.cat(y_list, dim=0)\n",
    "\n",
    "            if y_mean is not None and y_std is not None:\n",
    "                y_all = (y_all - y_mean) / y_std\n",
    "\n",
    "        self.head.compute_posterior(phi_all, y_all)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4795ab",
   "metadata": {
    "id": "0f4795ab"
   },
   "outputs": [],
   "source": [
    "# Deterministic EfficientNet regressor baseline\n",
    "class DeterministicRegressor(nn.Module):\n",
    "    \"\"\"Uses the same EfficientNet backbone but with a single linear head (no Bayesian posterior).\"\"\"\n",
    "    def __init__(self, backbone: nn.Module, num_outputs: int = 3):\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "        # Infer basis dimension\n",
    "        with torch.no_grad():\n",
    "            dummy = torch.randn(1, 3, 224, 224)\n",
    "            basis = self.backbone(dummy)\n",
    "            basis_dim = basis.shape[1]\n",
    "        self.head = nn.Linear(basis_dim, num_outputs)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        phi = self.backbone(x)\n",
    "        return self.head(phi)\n",
    "\n",
    "\n",
    "def train_deterministic(\n",
    "    model: DeterministicRegressor,\n",
    "    loader: DataLoader,\n",
    "    device: torch.device,\n",
    "    y_mean: torch.Tensor,\n",
    "    y_std: torch.Tensor,\n",
    "    loss_fn: Optional[Callable],\n",
    "    epochs: int = 10,\n",
    "    lr: float = 1e-3,\n",
    "    weight_decay: float = 1e-4,\n",
    "    subset_frac: float | None = None,\n",
    "    seed: int = 271,\n",
    " ):\n",
    "    if subset_frac is not None:\n",
    "        full_dataset = loader.dataset\n",
    "        full_size = len(full_dataset)\n",
    "        new_size = int(subset_frac*full_size)\n",
    "        g = torch.Generator().manual_seed(seed)\n",
    "        subset_idx = torch.randperm(full_size, generator=g)[:new_size]\n",
    "        loader = DataLoader(\n",
    "            Subset(full_dataset, subset_idx),\n",
    "            batch_size=loader.batch_size,\n",
    "            shuffle=False,  # keep the sampled set fixed\n",
    "            num_workers=getattr(loader, \"num_workers\", 0),\n",
    "            pin_memory=getattr(loader, \"pin_memory\", False),\n",
    "        )\n",
    "    model.train()\n",
    "    optimizer = torch.optim.Adam(model.head.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    if loss_fn is None:\n",
    "        loss_fn = nn.L1Loss()\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0.0\n",
    "        for batch_x, batch_y in loader:\n",
    "            batch_x = batch_x.to(device)\n",
    "            batch_y = batch_y.to(device)\n",
    "            y_std_safe = y_std.to(device)\n",
    "            y_mean_safe = y_mean.to(device)\n",
    "            targets_std = (batch_y - y_mean_safe) / y_std_safe\n",
    "            preds_std = model(batch_x)\n",
    "            loss = loss_fn(preds_std, targets_std)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item() * batch_x.size(0)\n",
    "        epoch_loss /= len(loader.dataset)\n",
    "    return epoch_loss\n",
    "\n",
    "\n",
    "def evaluate_deterministic(model, loader, device, y_mean, y_std):\n",
    "    model.eval()\n",
    "    preds, targets = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_y in loader:\n",
    "            batch_x = batch_x.to(device)\n",
    "            batch_y = batch_y.to(device)\n",
    "            preds_std = model(batch_x)\n",
    "            preds.append(preds_std)\n",
    "            targets.append(batch_y)\n",
    "    preds = torch.cat(preds, dim=0)\n",
    "    targets = torch.cat(targets, dim=0)\n",
    "    preds = preds * y_std + y_mean\n",
    "    mse = torch.mean((preds - targets) ** 2).item()\n",
    "    rmse = math.sqrt(mse)\n",
    "    mae = torch.mean(torch.abs(preds - targets)).item()\n",
    "    return {\"mse\": mse, \"rmse\": rmse, \"mae\": mae}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613789e5",
   "metadata": {
    "id": "613789e5"
   },
   "outputs": [],
   "source": [
    "def variance_acq(model: BayesianEfficientNetModel, pool_loader: DataLoader, device: torch.device, n_query: int = 10, y_std: Optional[torch.Tensor] = None) -> list:\n",
    "    \"\"\"\n",
    "    Uncertainty sampling: select points with highest predictive variance.\n",
    "    Optionally scales variance back to original target units using y_std.\n",
    "    \"\"\"\n",
    "    model.backbone.eval()\n",
    "\n",
    "    all_variances = []\n",
    "    pool_indices = []\n",
    "    idx_counter = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_x, _ in pool_loader:\n",
    "            batch_x = batch_x.to(device)\n",
    "            _, uncertainties = model.predict_with_uncertainty(batch_x)\n",
    "            if y_std is not None:\n",
    "                scale = y_std.to(device) ** 2\n",
    "                uncertainties = uncertainties * scale\n",
    "            mean_var = torch.mean(uncertainties, dim=1)\n",
    "            all_variances.append(mean_var.cpu())\n",
    "            batch_size = batch_x.size(0)\n",
    "            pool_indices.extend(range(idx_counter, idx_counter + batch_size))\n",
    "            idx_counter += batch_size\n",
    "    all_variances = torch.cat(all_variances, dim=0)\n",
    "    _, top_indices = torch.topk(all_variances, k=min(n_query, len(all_variances)))\n",
    "    selected = [pool_indices[i] for i in top_indices.tolist()]\n",
    "\n",
    "    return selected\n",
    "\n",
    "\n",
    "def random_acq(model: BayesianEfficientNetModel, pool_loader: DataLoader, device: torch.device, n_query: int = 10, y_std: Optional[torch.Tensor] = None) -> list:\n",
    "    \"\"\"\n",
    "    Uniform random sampling: select n_query points randomly.\n",
    "    \"\"\"\n",
    "    total_pool_size = 0\n",
    "    for batch_x, _ in pool_loader:\n",
    "        total_pool_size += batch_x.size(0)\n",
    "\n",
    "    selected = list(np.random.choice(total_pool_size, size=min(n_query, total_pool_size), replace=False))\n",
    "    return selected\n",
    "\n",
    "def max_entropy_acq(model: BayesianEfficientNetModel, pool_loader: DataLoader, device: torch.device, n_query: int = 10, y_std: Optional[torch.Tensor] = None) -> list:\n",
    "    \"\"\"\n",
    "    Select samples with the highest predictive differential entropy assuming\n",
    "    independent Gaussian outputs. Optionally rescales variance using y_std.\n",
    "    \"\"\"\n",
    "    model.backbone.eval()\n",
    "    entropies = []\n",
    "    pool_indices = []\n",
    "    idx_counter = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_x, _ in pool_loader:\n",
    "            batch_x = batch_x.to(device)\n",
    "            _, variances = model.predict_with_uncertainty(batch_x)\n",
    "            if y_std is not None:\n",
    "                scale = y_std.to(device) ** 2\n",
    "                variances = variances * scale\n",
    "            variances = torch.clamp(variances, min=1e-8)\n",
    "            entropy = 0.5 * torch.sum(torch.log(2 * math.pi * math.e * variances), dim=1)\n",
    "            entropies.append(entropy.cpu())\n",
    "            batch_size = batch_x.size(0)\n",
    "            pool_indices.extend(range(idx_counter, idx_counter + batch_size))\n",
    "            idx_counter += batch_size\n",
    "    entropies = torch.cat(entropies, dim=0)\n",
    "    _, top_indices = torch.topk(entropies, k=min(n_query, len(entropies)))\n",
    "    selected = [pool_indices[i] for i in top_indices.tolist()]\n",
    "    return selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092cd82f",
   "metadata": {
    "id": "092cd82f"
   },
   "outputs": [],
   "source": [
    "def evaluate_model(model: BayesianEfficientNetModel, loader: DataLoader, device: torch.device, y_mean: Optional[torch.Tensor] = None, y_std: Optional[torch.Tensor] = None) -> dict:\n",
    "    model.backbone.eval()\n",
    "    predictions = []\n",
    "    targets = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_y in loader:\n",
    "            batch_x = batch_x.to(device)\n",
    "            batch_y = batch_y.to(device)\n",
    "            preds, _ = model.predict_with_uncertainty(batch_x)\n",
    "            predictions.append(preds.cpu())\n",
    "            targets.append(batch_y.cpu())\n",
    "    predictions = torch.cat(predictions, dim=0)\n",
    "    targets = torch.cat(targets, dim=0)\n",
    "    if y_mean is not None and y_std is not None:\n",
    "        predictions = predictions * y_std.cpu() + y_mean.cpu()\n",
    "    mse = torch.mean((predictions - targets) ** 2).item()\n",
    "    rmse = math.sqrt(mse)\n",
    "    mae = torch.mean(torch.abs(predictions - targets)).item()\n",
    "    return {\"mse\": mse, \"rmse\": rmse, \"mae\": mae}\n",
    "\n",
    "\n",
    "def active_learning_loop(\n",
    "    model: BayesianEfficientNetModel,\n",
    "    dataset: Dataset,\n",
    "    idx_labeled: torch.Tensor,\n",
    "    idx_pool: torch.Tensor,\n",
    "    val_loader: DataLoader,\n",
    "    device: torch.device,\n",
    "    acquisition_fn,\n",
    "    acq_rounds: int = 10,\n",
    "    n_query: int = 10,\n",
    "    batch_size: int = 64,\n",
    "    test_loader: Optional[DataLoader] = None,\n",
    ") -> dict:\n",
    "    \"\"\"Active learning loop: iteratively select and label points from pool.\"\"\"\n",
    "    history = {\n",
    "        \"iteration\": [],\n",
    "        \"n_labeled\": [],\n",
    "        \"val_mse\": [],\n",
    "        \"val_rmse\": [],\n",
    "        \"val_mae\": [],\n",
    "    }\n",
    "\n",
    "    for iteration in range(acq_rounds):\n",
    "        labeled_subset = Subset(dataset, idx_labeled.tolist())\n",
    "        stats_loader = DataLoader(labeled_subset, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
    "        y_mean, y_std = compute_label_stats(stats_loader, device)\n",
    "        labeled_loader = DataLoader(labeled_subset, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
    "        model.fit_posterior(labeled_loader, device, y_mean=y_mean, y_std=y_std)\n",
    "        pool_subset = Subset(dataset, idx_pool.tolist())\n",
    "        pool_loader = DataLoader(pool_subset, batch_size=64, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "        selected_pool_indices = acquisition_fn(model, pool_loader, device, n_query=n_query, y_std=y_std)\n",
    "        selected_full_indices = [idx_pool[i].item() for i in selected_pool_indices]\n",
    "\n",
    "        idx_labeled = torch.cat([idx_labeled, torch.tensor(selected_full_indices, dtype=idx_labeled.dtype)])\n",
    "        idx_pool = torch.tensor([i for i in idx_pool.tolist() if i not in selected_full_indices], dtype=idx_pool.dtype)\n",
    "\n",
    "        val_metrics = evaluate_model(model, val_loader, device, y_mean=y_mean, y_std=y_std)\n",
    "\n",
    "        history[\"iteration\"].append(iteration + 1)\n",
    "        history[\"n_labeled\"].append(len(idx_labeled))\n",
    "        history[\"val_mse\"].append(val_metrics[\"mse\"])\n",
    "        history[\"val_rmse\"].append(val_metrics[\"rmse\"])\n",
    "        history[\"val_mae\"].append(val_metrics[\"mae\"])\n",
    "        print(f\"Iteration {iteration + 1}/{acq_rounds} | Labeled set size: {len(idx_labeled)} | Val RMSE: {val_metrics['rmse']:.3f} | Val MAE: {val_metrics['mae']:.3f}\")\n",
    "\n",
    "    if test_loader is not None:\n",
    "        final_stats_loader = DataLoader(Subset(dataset, idx_labeled.tolist()), batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
    "        y_mean_final, y_std_final = compute_label_stats(final_stats_loader, device)\n",
    "        test_metrics = evaluate_model(model, test_loader, device, y_mean=y_mean_final, y_std=y_std_final)\n",
    "        history[\"final_test_rmse\"] = test_metrics[\"rmse\"]\n",
    "        history[\"final_test_mae\"] = test_metrics[\"mae\"]\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a093a676",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a093a676",
    "outputId": "ceac74ad-4ffe-42f1-a93e-1062eff1baa3"
   },
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"RUNNING ALL ACTIVE LEARNING EXPERIMENTS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "experiments = {}\n",
    "\n",
    "# Configuration for all experiments\n",
    "args = argparse.Namespace(\n",
    "    seed = 271,\n",
    "    acq_rounds=110,\n",
    "    n_query=25,\n",
    "    batch_size=128,\n",
    "    pool_ratio=0.05,\n",
    "    pool_size=5000,\n",
    "    val_size=500,\n",
    "    test_size=1000,\n",
    "    likelihood_variance=1.0,\n",
    "    prior_variance=1.0,\n",
    ")\n",
    "\n",
    "model_configs = [\n",
    "    (\"mfvi\", \"MFVI\"),\n",
    "    (\"analytical\", \"Analytical\"),\n",
    "]\n",
    "\n",
    "acquisition_configs = [\n",
    "    (variance_acq, \"Variance\"),\n",
    "    (random_acq, \"Random\"),\n",
    "]\n",
    "set_seed(args.seed)\n",
    "(init_loader, pool_loader, determ_train_loader, val_loader, test_loader), pool_dataset, (idx_labeled, idx_pool) = prep_data(\n",
    "    seed=args.seed,\n",
    "    pool_ratio=args.pool_ratio,\n",
    "    pool_size=args.pool_size,\n",
    "    val_size=args.val_size,\n",
    "    test_size=args.test_size,\n",
    ")\n",
    "for head_type, head_label in model_configs:\n",
    "    for acq_fn, acq_label in acquisition_configs:\n",
    "        exp_name = f\"{head_label}_{acq_label}\"\n",
    "        # if exp_name in experiments.keys():\n",
    "        #     print(f'Experiment results already available for: {exp_name}')\n",
    "        #     continue\n",
    "        # else:\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"Experiment: {exp_name}\")\n",
    "        print(f\"{'='*70}\")\n",
    "\n",
    "        # Create fresh model\n",
    "        backbone = EfficientNetBasis()\n",
    "        model = BayesianEfficientNetModel(\n",
    "            backbone=backbone,\n",
    "            num_outputs=3,\n",
    "            head_type=head_type,\n",
    "            likelihood_variance=args.likelihood_variance,\n",
    "            prior_variance=args.prior_variance,\n",
    "        ).to(device)\n",
    "\n",
    "        # Create fresh index tensors\n",
    "        idx_labeled_al = idx_labeled.clone()\n",
    "        idx_pool_al = idx_pool.clone()\n",
    "\n",
    "        history = active_learning_loop(\n",
    "            model=model,\n",
    "            dataset=pool_dataset,\n",
    "            idx_labeled=idx_labeled_al,\n",
    "            idx_pool=idx_pool_al,\n",
    "            val_loader=val_loader,\n",
    "            device=device,\n",
    "            acquisition_fn=acq_fn,\n",
    "            test_loader=test_loader,\n",
    "            acq_rounds=args.acq_rounds,\n",
    "            n_query = args.n_query,\n",
    "            batch_size=args.batch_size,\n",
    "        )\n",
    "        experiments[exp_name] = history\n",
    "        final_val_mae = history[\"val_mae\"][-1]\n",
    "        final_val_rmse = history[\"val_rmse\"][-1]\n",
    "        final_test_mae = history.get(\"final_test_mae\")\n",
    "        final_test_rmse = history.get(\"final_test_rmse\")\n",
    "        print(f\"Final Val MAE: {final_val_mae:.4f}, Final Val RMSE: {final_val_rmse:.4f}\")\n",
    "        if final_test_mae is not None:\n",
    "            print(f\"Final Test MAE: {final_test_mae:.4f}, Final Test RMSE: {final_test_rmse:.4f}\")\n",
    "            print(f\"Labeled samples: {history['n_labeled'][-1]}\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"All experiments completed!\")\n",
    "print(f\"{'='*70}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "PiVzN4tCZBwq",
   "metadata": {
    "id": "PiVzN4tCZBwq"
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"RUNNING DETERMINISTIC REGRESSOR (RANDOM SELECTION)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Match sample sizes to AL checkpoints\n",
    "init_size = len(idx_labeled)\n",
    "final_al_size = init_size + args.acq_rounds * args.n_query\n",
    "total_determ_data = len(determ_train_loader.dataset)\n",
    "\n",
    "# Create checkpoints matching AL progression\n",
    "checkpoint_interval = 10\n",
    "det_sample_sizes = [init_size + i * args.n_query * checkpoint_interval\n",
    "                    for i in range(args.acq_rounds // checkpoint_interval + 1)]\n",
    "\n",
    "\n",
    "y_mean_det, y_std_det = compute_label_stats(determ_train_loader, device)\n",
    "for loss_fn in ['L1','MSE']:\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Experiment: Deterministic_{loss_fn}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    det_history = {\n",
    "        \"iteration\": [],\n",
    "        \"n_labeled\": [],\n",
    "        \"val_mse\": [],\n",
    "        \"val_rmse\": [],\n",
    "        \"val_mae\": [],\n",
    "    }\n",
    "    for idx, sample_size in enumerate(det_sample_sizes):\n",
    "        subset_frac = sample_size / total_determ_data\n",
    "        backbone_det = EfficientNetBasis()\n",
    "        det_model = DeterministicRegressor(backbone_det, num_outputs=3).to(device)\n",
    "        if loss_fn == 'L1':\n",
    "            final_loss = train_deterministic(\n",
    "                det_model,\n",
    "                determ_train_loader,\n",
    "                device,\n",
    "                y_mean_det,\n",
    "                y_std_det,\n",
    "                loss_fn=nn.L1Loss(),\n",
    "                epochs=50,\n",
    "                lr=1e-3,\n",
    "                weight_decay=1e-4,\n",
    "                subset_frac=subset_frac,\n",
    "                seed=args.seed + idx\n",
    "            )\n",
    "        else:\n",
    "            final_loss = train_deterministic(\n",
    "                det_model,\n",
    "                determ_train_loader,\n",
    "                device,\n",
    "                y_mean_det,\n",
    "                y_std_det,\n",
    "                loss_fn=nn.MSELoss(),\n",
    "                epochs=30,\n",
    "                lr=1e-3,\n",
    "                weight_decay=1e-4,\n",
    "                subset_frac=subset_frac,\n",
    "                seed=args.seed + idx\n",
    "            )\n",
    "        val_metrics_det = evaluate_deterministic(det_model, val_loader, device, y_mean_det, y_std_det)\n",
    "\n",
    "        det_history[\"iteration\"].append(idx + 1)\n",
    "        det_history[\"n_labeled\"].append(sample_size)\n",
    "        det_history[\"val_mse\"].append(val_metrics_det[\"mse\"])\n",
    "        det_history[\"val_rmse\"].append(val_metrics_det[\"rmse\"])\n",
    "        det_history[\"val_mae\"].append(val_metrics_det[\"mae\"])\n",
    "        print(f\"Iteration {idx + 1}/{len(det_sample_sizes)} | Labeled set size: {sample_size} | Val RMSE: {val_metrics_det['rmse']:.4f}, Val MAE: {val_metrics_det['mae']:.4f}\")\n",
    "\n",
    "    test_metrics_det = evaluate_deterministic(det_model, test_loader, device, y_mean_det, y_std_det)\n",
    "    det_history[\"final_test_rmse\"] = test_metrics_det[\"rmse\"]\n",
    "    det_history[\"final_test_mae\"] = test_metrics_det[\"mae\"]\n",
    "    print(f\"Final Val RMSE: {det_history['val_rmse'][-1]:.4f} | Final Val MAE: {det_history['val_mae'][-1]:.4f}\")\n",
    "    print(f\"Final Test RMSE: {test_metrics_det['rmse']:.4f} | Final Test MAE: {test_metrics_det['mae']:.4f}\")\n",
    "    print(f\"\\nDeterministic trained on: {det_history['n_labeled']} samples\")\n",
    "    experiments[f\"Deterministic_{loss_fn}\"] = det_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b9ffe3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 756
    },
    "id": "c5b9ffe3",
    "outputId": "a8a26b04-2957-4fdf-e609-35de10e05a1b"
   },
   "outputs": [],
   "source": [
    "# Plot validation metrics\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "tab20 = plt.cm.tab20.colors\n",
    "\n",
    "colors = {\n",
    "    \"Analytical_Variance\": tab20[0],\n",
    "    \"Analytical_Random\": tab20[1],\n",
    "    \"MFVI_Variance\": tab20[6],\n",
    "    \"MFVI_Random\": tab20[7],\n",
    "    \"Deterministic_L1\": tab20[4],\n",
    "    \"Deterministic_MSE\": tab20[5],\n",
    "}\n",
    "linestyles = {\n",
    "    \"Analytical_Variance\": \"-\",\n",
    "    \"Analytical_Random\": \"--\",\n",
    "    \"MFVI_Variance\": \"-\",\n",
    "    \"MFVI_Random\": \"--\",\n",
    "    \"Deterministic_L1\": \"-\",\n",
    "    \"Deterministic_MSE\": \"--\",\n",
    "}\n",
    "markers = {\n",
    "    \"Analytical_Variance\": None,\n",
    "    \"Analytical_Random\": None,\n",
    "    \"MFVI_Variance\": None,\n",
    "    \"MFVI_Random\": None,\n",
    "    \"Deterministic_L1\": \"s\",\n",
    "    \"Deterministic_MSE\": \"s\",\n",
    "}\n",
    "\n",
    "ax = axes[0]\n",
    "for exp_name, history in experiments.items():\n",
    "    ax.plot(\n",
    "        history[\"n_labeled\"],\n",
    "        history[\"val_mae\"],\n",
    "        color=colors.get(exp_name, \"tab:gray\"),\n",
    "        linestyle=linestyles.get(exp_name, \"-\"),\n",
    "        marker=markers.get(exp_name, None),\n",
    "        markersize=4 if markers.get(exp_name) else 0,\n",
    "        linewidth=2.5,\n",
    "        label=exp_name.replace(\"_\", \" | \")\n",
    "    )\n",
    "ax.set_xlabel(\"# Labeled Samples\")\n",
    "ax.set_ylabel(\"Validation MAE\")\n",
    "ax.set_title(\"Validation MAE vs Labeled Samples\")\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.legend()\n",
    "\n",
    "ax = axes[1]\n",
    "for exp_name, history in experiments.items():\n",
    "    ax.plot(\n",
    "        history[\"n_labeled\"],\n",
    "        history[\"val_rmse\"],\n",
    "        color=colors.get(exp_name, \"tab:gray\"),\n",
    "        linestyle=linestyles.get(exp_name, \"-\"),\n",
    "        marker=markers.get(exp_name, None),\n",
    "        markersize=4 if markers.get(exp_name) else 0,\n",
    "        linewidth=2.5,\n",
    "        label=exp_name.replace(\"_\", \" | \")\n",
    "    )\n",
    "ax.set_xlabel(\"# Labeled Samples\")\n",
    "ax.set_ylabel(\"Validation RMSE\")\n",
    "ax.set_title(\"Validation RMSE vs Labeled Samples\")\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"novel_extension.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "# Summary table based on MAE and RMSE\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SUMMARY OF ALL EXPERIMENTS (MAE / RMSE)\")\n",
    "print(\"=\"*80)\n",
    "print(f\"{'Configuration':<30} {'Final Val MAE':<15} {'Final Val RMSE':<17} {'Final Test MAE':<15} {'Final Test RMSE':<17}\")\n",
    "print(\"-\"*80)\n",
    "for exp_name, history in sorted(experiments.items()):\n",
    "    final_val_mae = history[\"val_mae\"][-1]\n",
    "    final_val_rmse = history[\"val_rmse\"][-1]\n",
    "    final_test_mae = history.get(\"final_test_mae\", float('nan'))\n",
    "    final_test_rmse = history.get(\"final_test_rmse\", float('nan'))\n",
    "    print(\n",
    "        f\"{exp_name:<30} \"\n",
    "        f\"{final_val_mae:<15.4f} \"\n",
    "        f\"{final_val_rmse:<17.4f} \"\n",
    "        f\"{final_test_mae:<15.4f} \"\n",
    "        f\"{final_test_rmse:<17.4f}\"\n",
    "    )\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
