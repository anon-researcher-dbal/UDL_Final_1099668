{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1831d6d8",
      "metadata": {
        "id": "1831d6d8"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import argparse\n",
        "import random\n",
        "import time\n",
        "from typing import Callable, Dict, List, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import MNIST\n",
        "from scipy import stats\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def set_seed(seed: int) -> None:\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c0124fab",
      "metadata": {
        "id": "c0124fab"
      },
      "source": [
        "## 2. Model Definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "710d51f7",
      "metadata": {
        "id": "710d51f7"
      },
      "outputs": [],
      "source": [
        "class ConvNN(nn.Module):\n",
        "    \"\"\"Convolutional neural network with MC-dropout for active learning.\"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_filters: int = 32,\n",
        "        kernel_size: int = 4,\n",
        "        dense_layer: int = 128,\n",
        "        img_rows: int = 28,\n",
        "        img_cols: int = 28,\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(1, num_filters, kernel_size, stride=1)\n",
        "        self.conv2 = nn.Conv2d(num_filters, num_filters, kernel_size, stride=1)\n",
        "        self.dropout1 = nn.Dropout(0.25)\n",
        "        self.dropout2 = nn.Dropout(0.5)\n",
        "        pooled_h = (img_rows - 2 * kernel_size + 2) // 2\n",
        "        pooled_w = (img_cols - 2 * kernel_size + 2) // 2\n",
        "        self.fc1 = nn.Linear(num_filters * pooled_h * pooled_w, dense_layer)\n",
        "        self.fc2 = nn.Linear(dense_layer, 10)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.max_pool2d(x, 2)\n",
        "        x = self.dropout1(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout2(x)\n",
        "        return self.fc2(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8db3989f",
      "metadata": {
        "id": "8db3989f"
      },
      "source": [
        "## 3. Data Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "503e74e4",
      "metadata": {
        "id": "503e74e4"
      },
      "outputs": [],
      "source": [
        "class LoadData:\n",
        "    \"\"\"Download, split, and prepare MNIST for active learning.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        val_size: int = 100,\n",
        "        train_size: int = 10000,\n",
        "        seed: int = 271,\n",
        "        root: str = \"data\",\n",
        "    ) -> None:\n",
        "        self.train_size = train_size\n",
        "        self.val_size = val_size\n",
        "        self.seed = seed\n",
        "        self.root = root\n",
        "        self.mnist_train, self.mnist_test = self.download_dataset()\n",
        "        self.pool_size = len(self.mnist_train) - self.train_size - self.val_size\n",
        "        (\n",
        "            self.X_train_All,\n",
        "            self.y_train_All,\n",
        "            self.X_val,\n",
        "            self.y_val,\n",
        "            self.X_pool,\n",
        "            self.y_pool,\n",
        "            self.X_test,\n",
        "            self.y_test,\n",
        "        ) = self.split_and_load_dataset()\n",
        "        self.X_init, self.y_init = self.preprocess_training_data()\n",
        "\n",
        "    def tensor_to_np(self, tensor_data: torch.Tensor) -> np.ndarray:\n",
        "        return tensor_data.detach().cpu().numpy()\n",
        "\n",
        "    def check_mnist_folder(self) -> bool:\n",
        "        return not os.path.exists(os.path.join(self.root, \"MNIST\"))\n",
        "\n",
        "    def download_dataset(self):\n",
        "        transform = transforms.Compose(\n",
        "            [transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]\n",
        "        )\n",
        "        download = self.check_mnist_folder()\n",
        "        mnist_train = MNIST(\n",
        "            self.root, train=True, download=download, transform=transform\n",
        "        )\n",
        "        mnist_test = MNIST(\n",
        "            self.root, train=False, download=download, transform=transform\n",
        "        )\n",
        "        return mnist_train, mnist_test\n",
        "\n",
        "    def split_and_load_dataset(self):\n",
        "        generator = torch.Generator().manual_seed(self.seed)\n",
        "        train_set, val_set, pool_set = random_split(\n",
        "            self.mnist_train,\n",
        "            [self.train_size, self.val_size, self.pool_size],\n",
        "            generator=generator,\n",
        "        )\n",
        "        train_loader = DataLoader(\n",
        "            dataset=train_set, batch_size=self.train_size, shuffle=True\n",
        "        )\n",
        "        val_loader = DataLoader(\n",
        "            dataset=val_set, batch_size=self.val_size, shuffle=True\n",
        "        )\n",
        "        pool_loader = DataLoader(\n",
        "            dataset=pool_set, batch_size=self.pool_size, shuffle=True\n",
        "        )\n",
        "        test_loader = DataLoader(\n",
        "            dataset=self.mnist_test, batch_size=10000, shuffle=True\n",
        "        )\n",
        "        X_train_All, y_train_All = next(iter(train_loader))\n",
        "        X_val, y_val = next(iter(val_loader))\n",
        "        X_pool, y_pool = next(iter(pool_loader))\n",
        "        X_test, y_test = next(iter(test_loader))\n",
        "        return (\n",
        "            X_train_All,\n",
        "            y_train_All,\n",
        "            X_val,\n",
        "            y_val,\n",
        "            X_pool,\n",
        "            y_pool,\n",
        "            X_test,\n",
        "            y_test,\n",
        "        )\n",
        "\n",
        "    def preprocess_training_data(self):\n",
        "        initial_idx: np.ndarray = np.array([], dtype=int)\n",
        "        for i in range(10):\n",
        "            candidates = np.where(self.y_train_All.numpy() == i)[0]\n",
        "            idx = np.random.choice(candidates, size=2, replace=False)\n",
        "            initial_idx = np.concatenate((initial_idx, idx))\n",
        "        X_init = self.X_train_All[initial_idx]\n",
        "        y_init = self.y_train_All[initial_idx]\n",
        "        print(f\"Initial training data points: {X_init.shape[0]}\")\n",
        "        print(f\"Data distribution for each class: {np.bincount(y_init.numpy())}\")\n",
        "        return X_init, y_init\n",
        "\n",
        "    def load_all(self):\n",
        "        return (\n",
        "            self.tensor_to_np(self.X_init),\n",
        "            self.tensor_to_np(self.y_init),\n",
        "            self.tensor_to_np(self.X_val),\n",
        "            self.tensor_to_np(self.y_val),\n",
        "            self.tensor_to_np(self.X_pool),\n",
        "            self.tensor_to_np(self.y_pool),\n",
        "            self.tensor_to_np(self.X_test),\n",
        "            self.tensor_to_np(self.y_test),\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e925a8d7",
      "metadata": {
        "id": "e925a8d7"
      },
      "source": [
        "## 4. Acquisition Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "94b7bcb0",
      "metadata": {
        "id": "94b7bcb0"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def _forward_probs(\n",
        "    model: nn.Module,\n",
        "    batch: torch.Tensor,\n",
        "    training: bool,\n",
        ") -> torch.Tensor:\n",
        "    \"\"\"Run forward pass with optional MC-dropout.\"\"\"\n",
        "    if training:\n",
        "        model.train()\n",
        "    else:\n",
        "        model.eval()\n",
        "    logits = model(batch)\n",
        "    return torch.softmax(logits, dim=-1)\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def predictions_from_pool(\n",
        "    model: nn.Module,\n",
        "    X_pool: np.ndarray,\n",
        "    T: int = 100,\n",
        "    training: bool = True,\n",
        "    subset_size: int = 2000,\n",
        "    device: torch.device | str | None = None,\n",
        ") -> Tuple[np.ndarray, np.ndarray]:\n",
        "    \"\"\"MC-dropout predictions on random subset of pool.\"\"\"\n",
        "    subset_size = min(subset_size, len(X_pool))\n",
        "    random_subset = np.random.choice(\n",
        "        range(len(X_pool)), size=subset_size, replace=False\n",
        "    )\n",
        "    x_tensor = torch.from_numpy(X_pool[random_subset]).to(device)\n",
        "    outputs = [\n",
        "        _forward_probs(model, x_tensor, training=training).cpu().numpy()\n",
        "        for _ in range(T)\n",
        "    ]\n",
        "    return np.stack(outputs), random_subset\n",
        "\n",
        "\n",
        "def uniform(\n",
        "    model: nn.Module,\n",
        "    X_pool: np.ndarray,\n",
        "    n_query: int = 10,\n",
        "    **_: object,\n",
        ") -> Tuple[np.ndarray, np.ndarray]:\n",
        "    \"\"\"Uniformly sample points from pool.\"\"\"\n",
        "    n_query = min(n_query, len(X_pool))\n",
        "    query_idx = np.random.choice(range(len(X_pool)), size=n_query, replace=False)\n",
        "    return query_idx, X_pool[query_idx]\n",
        "\n",
        "\n",
        "def shannon_entropy_function(\n",
        "    model: nn.Module,\n",
        "    X_pool: np.ndarray,\n",
        "    T: int = 100,\n",
        "    training: bool = True,\n",
        "    device: torch.device | str | None = None,\n",
        ") -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
        "    \"\"\"Compute entropy H and expected entropy E, return with random subset.\"\"\"\n",
        "    outputs, random_subset = predictions_from_pool(\n",
        "        model, X_pool, T=T, training=training, device=device\n",
        "    )\n",
        "    pc = outputs.mean(axis=0)\n",
        "    H = (-pc * np.log(pc + 1e-10)).sum(axis=-1)\n",
        "    E = -np.mean(np.sum(outputs * np.log(outputs + 1e-10), axis=-1), axis=0)\n",
        "    return H, E, random_subset\n",
        "\n",
        "\n",
        "def max_entropy(\n",
        "    model: nn.Module,\n",
        "    X_pool: np.ndarray,\n",
        "    n_query: int = 10,\n",
        "    T: int = 100,\n",
        "    training: bool = True,\n",
        "    device: torch.device | str | None = None,\n",
        ") -> Tuple[np.ndarray, np.ndarray]:\n",
        "    H, _, random_subset = shannon_entropy_function(\n",
        "        model, X_pool, T=T, training=training, device=device\n",
        "    )\n",
        "    idx = (-H).argsort()[:n_query]\n",
        "    query_idx = random_subset[idx]\n",
        "    return query_idx, X_pool[query_idx]\n",
        "\n",
        "\n",
        "def bald(\n",
        "    model: nn.Module,\n",
        "    X_pool: np.ndarray,\n",
        "    n_query: int = 10,\n",
        "    T: int = 100,\n",
        "    training: bool = True,\n",
        "    device: torch.device | str | None = None,\n",
        ") -> Tuple[np.ndarray, np.ndarray]:\n",
        "    H, E, random_subset = shannon_entropy_function(\n",
        "        model, X_pool, T=T, training=training, device=device\n",
        "    )\n",
        "    acquisition = H - E\n",
        "    idx = (-acquisition).argsort()[:n_query]\n",
        "    query_idx = random_subset[idx]\n",
        "    return query_idx, X_pool[query_idx]\n",
        "\n",
        "\n",
        "def var_ratios(\n",
        "    model: nn.Module,\n",
        "    X_pool: np.ndarray,\n",
        "    n_query: int = 10,\n",
        "    T: int = 100,\n",
        "    training: bool = True,\n",
        "    device: torch.device | str | None = None,\n",
        ") -> Tuple[np.ndarray, np.ndarray]:\n",
        "    outputs, random_subset = predictions_from_pool(\n",
        "        model, X_pool, T=T, training=training, device=device\n",
        "    )\n",
        "    preds = np.argmax(outputs, axis=2)\n",
        "    _, count = stats.mode(preds, axis=0, keepdims=False)\n",
        "    acquisition = (1 - count / preds.shape[0]).reshape((-1,))\n",
        "    idx = (-acquisition).argsort()[:n_query]\n",
        "    query_idx = random_subset[idx]\n",
        "    return query_idx, X_pool[query_idx]\n",
        "\n",
        "\n",
        "def mean_std(\n",
        "    model: nn.Module,\n",
        "    X_pool: np.ndarray,\n",
        "    n_query: int = 10,\n",
        "    T: int = 100,\n",
        "    training: bool = True,\n",
        "    device: torch.device | str | None = None,\n",
        ") -> Tuple[np.ndarray, np.ndarray]:\n",
        "    outputs, random_subset = predictions_from_pool(\n",
        "        model, X_pool, T=T, training=training, device=device\n",
        "    )\n",
        "    sigma_c = np.std(outputs, axis=0)\n",
        "    acquisition = np.mean(sigma_c, axis=-1)\n",
        "    idx = (-acquisition).argsort()[:n_query]\n",
        "    query_idx = random_subset[idx]\n",
        "    return query_idx, X_pool[query_idx]\n",
        "\n",
        "\n",
        "def select_acq_function(acq_func: int = 0):\n",
        "    \"\"\"Select acquisition functions by ID.\n",
        "    0: all, 1: uniform, 2: max_entropy, 3: bald, 4: var_ratios, 5: mean_std\n",
        "    \"\"\"\n",
        "    acq_func_dict = {\n",
        "        0: [uniform, max_entropy, bald, var_ratios, mean_std],\n",
        "        1: [uniform],\n",
        "        2: [max_entropy],\n",
        "        3: [bald],\n",
        "        4: [var_ratios],\n",
        "        5: [mean_std],\n",
        "    }\n",
        "    return acq_func_dict[acq_func]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aecf2de6",
      "metadata": {
        "id": "aecf2de6"
      },
      "source": [
        "## 5. Training Utilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8b321298",
      "metadata": {
        "id": "8b321298"
      },
      "outputs": [],
      "source": [
        "def _make_loader(\n",
        "    X: np.ndarray, y: np.ndarray, batch_size: int, shuffle: bool = True\n",
        ") -> DataLoader:\n",
        "    dataset = TensorDataset(\n",
        "        torch.from_numpy(X).float(), torch.from_numpy(y).long()\n",
        "    )\n",
        "    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
        "\n",
        "\n",
        "def _train_model(\n",
        "    model: nn.Module,\n",
        "    loader: DataLoader,\n",
        "    epochs: int,\n",
        "    lr: float,\n",
        "    weight_decay: float,\n",
        "    device: torch.device,\n",
        ") -> nn.Module:\n",
        "    model.to(device)\n",
        "    optimizer = torch.optim.Adam(\n",
        "        model.parameters(), lr=lr, weight_decay=weight_decay\n",
        "    )\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    model.train()\n",
        "    for _ in range(epochs):\n",
        "        for xb, yb in loader:\n",
        "            xb, yb = xb.to(device), yb.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            logits = model(xb)\n",
        "            loss = criterion(logits, yb)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "    return model\n",
        "\n",
        "\n",
        "def _accuracy(\n",
        "    model: nn.Module, X: np.ndarray, y: np.ndarray, device: torch.device\n",
        ") -> float:\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        xb = torch.from_numpy(X).float().to(device)\n",
        "        yb = torch.from_numpy(y).long().to(device)\n",
        "        preds = torch.argmax(model(xb), dim=1)\n",
        "        return float((preds == yb).float().mean().cpu().item())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2aefa1c8",
      "metadata": {
        "id": "2aefa1c8"
      },
      "source": [
        "## 6. Active Learning Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "60f4ff01",
      "metadata": {
        "id": "60f4ff01"
      },
      "outputs": [],
      "source": [
        "def active_learning_procedure(\n",
        "    query_strategy: Callable,\n",
        "    X_val: np.ndarray,\n",
        "    y_val: np.ndarray,\n",
        "    X_test: np.ndarray,\n",
        "    y_test: np.ndarray,\n",
        "    X_pool: np.ndarray,\n",
        "    y_pool: np.ndarray,\n",
        "    X_init: np.ndarray,\n",
        "    y_init: np.ndarray,\n",
        "    build_model: Callable[[], nn.Module],\n",
        "    *,\n",
        "    T: int = 100,\n",
        "    n_query: int = 10,\n",
        "    training: bool = True,\n",
        "    batch_size: int = 128,\n",
        "    epochs: int = 50,\n",
        "    lr: float = 1e-3,\n",
        "    weight_decay: float = 1e-2,\n",
        "    device: torch.device,\n",
        ") -> Tuple[List[float], float]:\n",
        "    \"\"\"Run active learning loop. Returns history and final test accuracy.\"\"\"\n",
        "    model = build_model().to(device)\n",
        "    train_loader = _make_loader(X_init, y_init, batch_size, shuffle=True)\n",
        "    model = _train_model(\n",
        "        model,\n",
        "        train_loader,\n",
        "        epochs=epochs,\n",
        "        lr=lr,\n",
        "        weight_decay=weight_decay,\n",
        "        device=device,\n",
        "    )\n",
        "    perf_hist = [_accuracy(model, X_test, y_test, device=device)]\n",
        "\n",
        "    for index in range(T):\n",
        "        query_idx, _ = query_strategy(\n",
        "            model,\n",
        "            X_pool,\n",
        "            n_query=n_query,\n",
        "            T=T,\n",
        "            training=training,\n",
        "            device=device,\n",
        "        )\n",
        "\n",
        "        X_train = np.concatenate([X_init, X_pool[query_idx]], axis=0)\n",
        "        y_train = np.concatenate([y_init, y_pool[query_idx]], axis=0)\n",
        "        X_pool = np.delete(X_pool, query_idx, axis=0)\n",
        "        y_pool = np.delete(y_pool, query_idx, axis=0)\n",
        "\n",
        "        model = build_model().to(device)\n",
        "        train_loader = _make_loader(X_train, y_train, batch_size, shuffle=True)\n",
        "        model = _train_model(\n",
        "            model,\n",
        "            train_loader,\n",
        "            epochs=epochs,\n",
        "            lr=lr,\n",
        "            weight_decay=weight_decay,\n",
        "            device=device,\n",
        "        )\n",
        "\n",
        "        val_acc = _accuracy(model, X_val, y_val, device=device)\n",
        "        if (index + 1) % 5 == 0:\n",
        "            print(f\"Val Accuracy after query {index + 1}: {val_acc:0.4f}\")\n",
        "        perf_hist.append(val_acc)\n",
        "\n",
        "        X_init, y_init = X_train, y_train\n",
        "\n",
        "    final_test_acc = _accuracy(model, X_test, y_test, device=device)\n",
        "    print(f\"********** Test Accuracy per trial: {final_test_acc:.4f} **********\")\n",
        "    return perf_hist, final_test_acc"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7f97ddcc",
      "metadata": {
        "id": "7f97ddcc"
      },
      "source": [
        "## 7. Experiment Harness"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6100ef70",
      "metadata": {
        "id": "6100ef70"
      },
      "outputs": [],
      "source": [
        "def train_active_learning(\n",
        "    args, device: torch.device, datasets: dict\n",
        ") -> Dict[str, np.ndarray]:\n",
        "    \"\"\"Run active learning for configured acquisition functions.\"\"\"\n",
        "    acq_functions = select_acq_function(args.acq_func)\n",
        "    results: Dict[str, np.ndarray] = {}\n",
        "    state_loop = [True, False] if args.determ else [True]\n",
        "\n",
        "    for state in state_loop:\n",
        "        for acq_func in acq_functions:\n",
        "            avg_hist: List[List[float]] = []\n",
        "            test_scores: List[float] = []\n",
        "            acq_func_name = f\"{acq_func.__name__}-MC_dropout={state}\"\n",
        "            print(f\"\\n---------- Start {acq_func_name} training ----------\")\n",
        "            for e in range(args.trials):\n",
        "                set_seed(args.seed + e)\n",
        "                start_time = time.time()\n",
        "                print(f\"********** Trials: {e + 1}/{args.trials} **********\")\n",
        "                training_hist, test_score = active_learning_procedure(\n",
        "                    query_strategy=acq_func,\n",
        "                    X_val=datasets[\"X_val\"],\n",
        "                    y_val=datasets[\"y_val\"],\n",
        "                    X_test=datasets[\"X_test\"],\n",
        "                    y_test=datasets[\"y_test\"],\n",
        "                    X_pool=datasets[\"X_pool\"],\n",
        "                    y_pool=datasets[\"y_pool\"],\n",
        "                    X_init=datasets[\"X_init\"],\n",
        "                    y_init=datasets[\"y_init\"],\n",
        "                    build_model=ConvNN,\n",
        "                    T=args.dropout_iter,\n",
        "                    n_query=args.query,\n",
        "                    training=state,\n",
        "                    batch_size=args.batch_size,\n",
        "                    epochs=args.epochs,\n",
        "                    lr=args.lr,\n",
        "                    weight_decay=args.weight_decay,\n",
        "                    device=device,\n",
        "                )\n",
        "                avg_hist.append(training_hist)\n",
        "                test_scores.append(test_score)\n",
        "                elapsed = time.time() - start_time\n",
        "                h = int(elapsed // 3600)\n",
        "                m = int(elapsed % 3600 // 60)\n",
        "                s = int(elapsed % 60)\n",
        "                print(f\"********** Trial {e + 1} ({acq_func_name}): {h}:{m}:{s} **********\")\n",
        "            avg_hist_arr = np.average(np.array(avg_hist), axis=0)\n",
        "            avg_test = sum(test_scores) / len(test_scores)\n",
        "            print(f\"Average Test score for {acq_func_name}: {avg_test}\")\n",
        "            results[acq_func_name] = avg_hist_arr\n",
        "    print(\"--------------- Training Complete ---------------\")\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "03a3874a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "03a3874a",
        "outputId": "87d1c83e-e826-4071-fa5e-e18d03e9c7a7"
      },
      "outputs": [],
      "source": [
        "args = argparse.Namespace(\n",
        "    batch_size=128,\n",
        "    epochs=100,\n",
        "    lr=1e-3,\n",
        "    weight_decay=1e-2,\n",
        "    seed=271,\n",
        "    trials=3,\n",
        "    dropout_iter=100,\n",
        "    query=10,\n",
        "    acq_func=0,  # all acquisition functions\n",
        "    val_size=100,\n",
        "    determ=False,  # Bayesian only\n",
        "    result_dir=\"exp1_results\",\n",
        ")\n",
        "\n",
        "set_seed(args.seed)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "data_loader = LoadData(val_size=args.val_size, seed=args.seed)\n",
        "(\n",
        "    X_init,\n",
        "    y_init,\n",
        "    X_val,\n",
        "    y_val,\n",
        "    X_pool,\n",
        "    y_pool,\n",
        "    X_test,\n",
        "    y_test,\n",
        ") = data_loader.load_all()\n",
        "\n",
        "datasets = {\n",
        "    \"X_init\": X_init,\n",
        "    \"y_init\": y_init,\n",
        "    \"X_val\": X_val,\n",
        "    \"y_val\": y_val,\n",
        "    \"X_pool\": X_pool,\n",
        "    \"y_pool\": y_pool,\n",
        "    \"X_test\": X_test,\n",
        "    \"y_test\": y_test,\n",
        "}\n",
        "\n",
        "results = train_active_learning(args, device, datasets)\n",
        "print(\"\\nResults:\")\n",
        "for name, curve in results.items():\n",
        "    print(f\"{name}: {curve[-1]:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2001fb7e",
      "metadata": {
        "id": "2001fb7e"
      },
      "source": [
        "## 9. Plotting Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "84dc0318",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        },
        "id": "84dc0318",
        "outputId": "a6ad1ec8-317f-4ae0-fb6f-ca5585b30b7e"
      },
      "outputs": [],
      "source": [
        "tab10_colors = plt.cm.tab10.colors\n",
        "\n",
        "color_map = {\n",
        "    \"bald\": tab10_colors[0],         # blue\n",
        "    \"max_entropy\": tab10_colors[1],  # orange\n",
        "    \"uniform\": tab10_colors[2],      # green\n",
        "    \"var_ratios\": tab10_colors[3],   # red\n",
        "    \"mean_std\": tab10_colors[4],     # purple\n",
        "}\n",
        "\n",
        "friendly_labels = {\n",
        "    \"uniform-MC_dropout=True\": \"Uniform (Bayesian)\",\n",
        "    \"max_entropy-MC_dropout=True\": \"Max Entropy (Bayesian)\",\n",
        "    \"bald-MC_dropout=True\": \"BALD (Bayesian)\",\n",
        "    \"var_ratios-MC_dropout=True\": \"Variation Ratios (Bayesian)\",\n",
        "    \"mean_std-MC_dropout=True\": \"Mean STD (Bayesian)\",\n",
        "}\n",
        "\n",
        "\n",
        "def get_color(name: str) -> tuple:\n",
        "    \"\"\"Map acquisition function name to tab10 color.\"\"\"\n",
        "    if \"uniform\" in name:\n",
        "        return color_map[\"uniform\"]\n",
        "    elif \"max_entropy\" in name:\n",
        "        return color_map[\"max_entropy\"]\n",
        "    elif \"bald\" in name:\n",
        "        return color_map[\"bald\"]\n",
        "    elif \"var_ratios\" in name:\n",
        "        return color_map[\"var_ratios\"]\n",
        "    elif \"mean_std\" in name:\n",
        "        return color_map[\"mean_std\"]\n",
        "    return tab10_colors[5]\n",
        "\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "ax.set_facecolor(\"white\")\n",
        "fig.patch.set_facecolor(\"white\")\n",
        "\n",
        "init_size = 20\n",
        "query_size = args.query\n",
        "for name, curve in results.items():\n",
        "    label = friendly_labels.get(name, name)\n",
        "    color = get_color(name)\n",
        "    x_values = init_size + np.arange(len(curve)) * query_size\n",
        "    ax.plot(x_values, curve, label=label, color=color, linewidth=2.5)\n",
        "\n",
        "ax.set_xlabel(\"Number of acquired images\", fontsize=11)\n",
        "ax.set_ylabel(\"Accuracy\", fontsize=11)\n",
        "ax.set_title(\n",
        "    \"Active learning accuracy vs no. of acquired images (Experiment 1)\",\n",
        "    fontsize=12,\n",
        "    fontweight=\"bold\",\n",
        ")\n",
        "ax.legend(loc=\"best\", framealpha=0.95)\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('exp1_plot.png',format='png')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ebc5381",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ebc5381",
        "outputId": "823b226b-b28c-48b8-c551-36675087f0e0"
      },
      "outputs": [],
      "source": [
        "def compute_images_to_error_table(\n",
        "    results: Dict[str, np.ndarray], query_size: int = 10\n",
        ") -> None:\n",
        "    \"\"\"Compute and print table of images needed to reach error thresholds.\"\"\"\n",
        "    init_size = 20\n",
        "    error_thresholds = [0.10, 0.05]\n",
        "    accuracy_thresholds = [1 - e for e in error_thresholds]\n",
        "\n",
        "    func_map = {\n",
        "        \"bald\": \"BALD\",\n",
        "        \"var_ratios\": \"Var Ratios\",\n",
        "        \"max_entropy\": \"Max Ent\",\n",
        "        \"mean_std\": \"Mean STD\",\n",
        "        \"uniform\": \"Random\",\n",
        "    }\n",
        "\n",
        "    table_data = {func: [] for func in func_map.values()}\n",
        "\n",
        "    for error_threshold, acc_threshold in zip(error_thresholds, accuracy_thresholds):\n",
        "        for name, curve in results.items():\n",
        "            func_name = name.split(\"-\")[0]\n",
        "            if func_name in func_map:\n",
        "                display_name = func_map[func_name]\n",
        "                indices = np.where(curve >= acc_threshold)[0]\n",
        "                if len(indices) > 0:\n",
        "                    step = indices[0]\n",
        "                    n_images = init_size + step * query_size\n",
        "                    table_data[display_name].append(n_images)\n",
        "                else:\n",
        "                    table_data[display_name].append(None)\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"Table: Number of acquired images to reach model error thresholds on MNIST\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    header = \"% error  | \" + \" | \".join(func_map.values())\n",
        "    print(header)\n",
        "    print(\"-\" * len(header))\n",
        "\n",
        "    for i, error_pct in enumerate([10, 5]):\n",
        "        row = f\"{error_pct:>7}% | \"\n",
        "        values = []\n",
        "        for func in func_map.values():\n",
        "            val = table_data[func][i]\n",
        "            if val is not None:\n",
        "                values.append(f\"{val:>10}\")\n",
        "            else:\n",
        "                values.append(f\"{'N/A':>10}\")\n",
        "        row += \" | \".join(values)\n",
        "        print(row)\n",
        "\n",
        "    print(\"=\" * 70 + \"\\n\")\n",
        "\n",
        "compute_images_to_error_table(results, query_size=args.query)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
