{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "59f07850",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "59f07850",
        "outputId": "aed95301-cf6e-4ba8-fe5b-344a56031b11"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import argparse\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import MNIST\n",
        "from scipy import stats\n",
        "\n",
        "def set_seed(seed: int) -> None:\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "\n",
        "class ConvNN(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_filters: int = 32,\n",
        "        kernel_size: int = 4,\n",
        "        dense_layer: int = 128,\n",
        "        img_rows: int = 28,\n",
        "        img_cols: int = 28,\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(1, num_filters, kernel_size, stride=1)\n",
        "        self.conv2 = nn.Conv2d(num_filters, num_filters, kernel_size, stride=1)\n",
        "        self.dropout1 = nn.Dropout(0.25)\n",
        "        self.dropout2 = nn.Dropout(0.5)\n",
        "        pooled_h = (img_rows - 2 * kernel_size + 2) // 2\n",
        "        pooled_w = (img_cols - 2 * kernel_size + 2) // 2\n",
        "        self.fc1 = nn.Linear(num_filters * pooled_h * pooled_w, dense_layer)\n",
        "        self.fc2 = nn.Linear(dense_layer, 10)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.max_pool2d(x, 2)\n",
        "        x = self.dropout1(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout2(x)\n",
        "        return self.fc2(x)\n",
        "\n",
        "class LoadData:\n",
        "    \"\"\"Download, split, and prepare MNIST for active learning.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        val_size: int = 100,\n",
        "        train_size: int = 10000,\n",
        "        seed: int = 369,\n",
        "        root: str = \"data\",\n",
        "    ) -> None:\n",
        "        self.train_size = train_size\n",
        "        self.val_size = val_size\n",
        "        self.seed = seed\n",
        "        self.root = root\n",
        "        self.mnist_train, self.mnist_test = self.download_dataset()\n",
        "        self.pool_size = len(self.mnist_train) - self.train_size - self.val_size\n",
        "        (\n",
        "            self.X_train_All,\n",
        "            self.y_train_All,\n",
        "            self.X_val,\n",
        "            self.y_val,\n",
        "            self.X_pool,\n",
        "            self.y_pool,\n",
        "            self.X_test,\n",
        "            self.y_test,\n",
        "        ) = self.split_and_load_dataset()\n",
        "        self.X_init, self.y_init = self.preprocess_training_data()\n",
        "\n",
        "    def tensor_to_np(self, tensor_data: torch.Tensor) -> np.ndarray:\n",
        "        return tensor_data.detach().cpu().numpy()\n",
        "\n",
        "    def check_mnist_folder(self) -> bool:\n",
        "        return not os.path.exists(os.path.join(self.root, \"MNIST\"))\n",
        "\n",
        "    def download_dataset(self):\n",
        "        transform = transforms.Compose(\n",
        "            [transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]\n",
        "        )\n",
        "        download = self.check_mnist_folder()\n",
        "        mnist_train = MNIST(\n",
        "            self.root, train=True, download=download, transform=transform\n",
        "        )\n",
        "        mnist_test = MNIST(self.root, train=False, download=download, transform=transform)\n",
        "        return mnist_train, mnist_test\n",
        "\n",
        "    def split_and_load_dataset(self):\n",
        "        generator = torch.Generator().manual_seed(self.seed)\n",
        "        train_set, val_set, pool_set = random_split(\n",
        "            self.mnist_train,\n",
        "            [self.train_size, self.val_size, self.pool_size],\n",
        "            generator=generator,\n",
        "        )\n",
        "        train_loader = DataLoader(\n",
        "            dataset=train_set, batch_size=self.train_size, shuffle=True\n",
        "        )\n",
        "        val_loader = DataLoader(dataset=val_set, batch_size=self.val_size, shuffle=True)\n",
        "        pool_loader = DataLoader(\n",
        "            dataset=pool_set, batch_size=self.pool_size, shuffle=True\n",
        "        )\n",
        "        test_loader = DataLoader(dataset=self.mnist_test, batch_size=10000, shuffle=True)\n",
        "        X_train_All, y_train_All = next(iter(train_loader))\n",
        "        X_val, y_val = next(iter(val_loader))\n",
        "        X_pool, y_pool = next(iter(pool_loader))\n",
        "        X_test, y_test = next(iter(test_loader))\n",
        "        return X_train_All, y_train_All, X_val, y_val, X_pool, y_pool, X_test, y_test\n",
        "\n",
        "    def preprocess_training_data(self):\n",
        "        initial_idx: np.ndarray = np.array([], dtype=int)\n",
        "        for i in range(10):\n",
        "            candidates = np.where(self.y_train_All.numpy() == i)[0]\n",
        "            idx = np.random.choice(candidates, size=2, replace=False)\n",
        "            initial_idx = np.concatenate((initial_idx, idx))\n",
        "        X_init = self.X_train_All[initial_idx]\n",
        "        y_init = self.y_train_All[initial_idx]\n",
        "        print(f\"Initial training data points: {X_init.shape[0]}\")\n",
        "        print(f\"Data distribution for each class: {np.bincount(y_init.numpy())}\")\n",
        "        return X_init, y_init\n",
        "\n",
        "    def load_all(self):\n",
        "        return (\n",
        "            self.tensor_to_np(self.X_init),\n",
        "            self.tensor_to_np(self.y_init),\n",
        "            self.tensor_to_np(self.X_val),\n",
        "            self.tensor_to_np(self.y_val),\n",
        "            self.tensor_to_np(self.X_pool),\n",
        "            self.tensor_to_np(self.y_pool),\n",
        "            self.tensor_to_np(self.X_test),\n",
        "            self.tensor_to_np(self.y_test),\n",
        "        )\n",
        "\n",
        "@torch.no_grad()\n",
        "def _forward_probs(model: nn.Module, batch: torch.Tensor, training: bool) -> torch.Tensor:\n",
        "    if training:\n",
        "        model.train()\n",
        "    else:\n",
        "        model.eval()\n",
        "    logits = model(batch)\n",
        "    return torch.softmax(logits, dim=-1)\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def predictions_from_pool(\n",
        "    model: nn.Module,\n",
        "    X_pool: np.ndarray,\n",
        "    T: int = 100,\n",
        "    training: bool = True,\n",
        "    subset_size: int = 2000,\n",
        "    device: torch.device | str | None = None,\n",
        " ):\n",
        "    subset_size = min(subset_size, len(X_pool))\n",
        "    random_subset = np.random.choice(range(len(X_pool)), size=subset_size, replace=False)\n",
        "    x_tensor = torch.from_numpy(X_pool[random_subset]).to(device)\n",
        "    outputs = [\n",
        "        _forward_probs(model, x_tensor, training=training).cpu().numpy()\n",
        "        for _ in range(T)\n",
        "    ]\n",
        "    return np.stack(outputs), random_subset\n",
        "\n",
        "\n",
        "def uniform(model: nn.Module, X_pool: np.ndarray, n_query: int = 10, **_):\n",
        "    n_query = min(n_query, len(X_pool))\n",
        "    query_idx = np.random.choice(range(len(X_pool)), size=n_query, replace=False)\n",
        "    return query_idx, X_pool[query_idx]\n",
        "\n",
        "\n",
        "def shannon_entropy_function(\n",
        "    model: nn.Module,\n",
        "    X_pool: np.ndarray,\n",
        "    T: int = 100,\n",
        "    E_H: bool = False,\n",
        "    training: bool = True,\n",
        "    device: torch.device | str | None = None,\n",
        " ):\n",
        "    outputs, random_subset = predictions_from_pool(\n",
        "        model, X_pool, T=T, training=training, device=device\n",
        "    )\n",
        "    pc = outputs.mean(axis=0)\n",
        "    H = (-pc * np.log(pc + 1e-10)).sum(axis=-1)\n",
        "    if E_H:\n",
        "        E = -np.mean(np.sum(outputs * np.log(outputs + 1e-10), axis=-1), axis=0)\n",
        "        return H, E, random_subset\n",
        "    return H, random_subset\n",
        "\n",
        "\n",
        "def max_entropy(model: nn.Module, X_pool: np.ndarray, n_query: int = 10, T: int = 100, training: bool = True, device=None):\n",
        "    acquisition, random_subset = shannon_entropy_function(\n",
        "        model, X_pool, T=T, training=training, device=device\n",
        "    )\n",
        "    idx = (-acquisition).argsort()[:n_query]\n",
        "    query_idx = random_subset[idx]\n",
        "    return query_idx, X_pool[query_idx]\n",
        "\n",
        "\n",
        "def bald(model: nn.Module, X_pool: np.ndarray, n_query: int = 10, T: int = 100, training: bool = True, device=None):\n",
        "    H, E_H, random_subset = shannon_entropy_function(\n",
        "        model, X_pool, T=T, E_H=True, training=training, device=device\n",
        "    )\n",
        "    acquisition = H - E_H\n",
        "    idx = (-acquisition).argsort()[:n_query]\n",
        "    query_idx = random_subset[idx]\n",
        "    return query_idx, X_pool[query_idx]\n",
        "\n",
        "\n",
        "def var_ratios(model: nn.Module, X_pool: np.ndarray, n_query: int = 10, T: int = 100, training: bool = True, device=None):\n",
        "    outputs, random_subset = predictions_from_pool(\n",
        "        model, X_pool, T=T, training=training, device=device\n",
        "    )\n",
        "    preds = np.argmax(outputs, axis=2)\n",
        "    _, count = stats.mode(preds, axis=0, keepdims=False)\n",
        "    acquisition = (1 - count / preds.shape[0]).reshape((-1,))\n",
        "    idx = (-acquisition).argsort()[:n_query]\n",
        "    query_idx = random_subset[idx]\n",
        "    return query_idx, X_pool[query_idx]\n",
        "\n",
        "\n",
        "def mean_std(model: nn.Module, X_pool: np.ndarray, n_query: int = 10, T: int = 100, training: bool = True, device=None):\n",
        "    outputs, random_subset = predictions_from_pool(\n",
        "        model, X_pool, T=T, training=training, device=device\n",
        "    )\n",
        "    sigma_c = np.std(outputs, axis=0)\n",
        "    acquisition = np.mean(sigma_c, axis=-1)\n",
        "    idx = (-acquisition).argsort()[:n_query]\n",
        "    query_idx = random_subset[idx]\n",
        "    return query_idx, X_pool[query_idx]\n",
        "\n",
        "\n",
        "def select_acq_function(acq_func: int = 0):\n",
        "    acq_func_dict = {\n",
        "        0: [uniform, max_entropy, bald, var_ratios, mean_std],\n",
        "        1: [uniform],\n",
        "        2: [max_entropy],\n",
        "        3: [bald],\n",
        "        4: [var_ratios],\n",
        "        5: [mean_std],\n",
        "    }\n",
        "    return acq_func_dict[acq_func]\n",
        "\n",
        "\n",
        "def _make_loader(X: np.ndarray, y: np.ndarray, batch_size: int, shuffle: bool = True):\n",
        "    dataset = TensorDataset(torch.from_numpy(X).float(), torch.from_numpy(y).long())\n",
        "    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
        "\n",
        "\n",
        "def _train_model(\n",
        "    model: nn.Module,\n",
        "    loader: DataLoader,\n",
        "    epochs: int,\n",
        "    lr: float,\n",
        "    weight_decay: float,\n",
        "    device: torch.device,\n",
        "):\n",
        "    model.to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    model.train()\n",
        "    for _ in range(epochs):\n",
        "        for xb, yb in loader:\n",
        "            xb, yb = xb.to(device), yb.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            logits = model(xb)\n",
        "            loss = criterion(logits, yb)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "    return model\n",
        "\n",
        "\n",
        "def _accuracy(model: nn.Module, X: np.ndarray, y: np.ndarray, device: torch.device) -> float:\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        xb = torch.from_numpy(X).float().to(device)\n",
        "        yb = torch.from_numpy(y).long().to(device)\n",
        "        preds = torch.argmax(model(xb), dim=1)\n",
        "        return float((preds == yb).float().mean().cpu().item())\n",
        "\n",
        "\n",
        "def active_learning_procedure(\n",
        "    query_strategy,\n",
        "    X_val,\n",
        "    y_val,\n",
        "    X_test,\n",
        "    y_test,\n",
        "    X_pool,\n",
        "    y_pool,\n",
        "    X_init,\n",
        "    y_init,\n",
        "    build_model,\n",
        "    *,\n",
        "    T: int = 100,\n",
        "    n_query: int = 10,\n",
        "    training: bool = True,\n",
        "    batch_size: int = 128,\n",
        "    epochs: int = 50,\n",
        "    lr: float = 1e-3,\n",
        "    weight_decay: float = 1e-2,\n",
        "    device: torch.device,\n",
        "):\n",
        "    model = build_model().to(device)\n",
        "    train_loader = _make_loader(X_init, y_init, batch_size, shuffle=True)\n",
        "    model = _train_model(\n",
        "        model, train_loader, epochs=epochs, lr=lr, weight_decay=weight_decay, device=device\n",
        "    )\n",
        "\n",
        "    perf_hist = [_accuracy(model, X_test, y_test, device=device)]\n",
        "\n",
        "    for index in range(T):\n",
        "        query_idx, _ = query_strategy(\n",
        "            model,\n",
        "            X_pool,\n",
        "            n_query=n_query,\n",
        "            T=T,\n",
        "            training=training,\n",
        "            device=device,\n",
        "        )\n",
        "\n",
        "        X_train = np.concatenate([X_init, X_pool[query_idx]], axis=0)\n",
        "        y_train = np.concatenate([y_init, y_pool[query_idx]], axis=0)\n",
        "        X_pool = np.delete(X_pool, query_idx, axis=0)\n",
        "        y_pool = np.delete(y_pool, query_idx, axis=0)\n",
        "\n",
        "        model = build_model().to(device)\n",
        "        train_loader = _make_loader(X_train, y_train, batch_size, shuffle=True)\n",
        "        model = _train_model(\n",
        "            model,\n",
        "            train_loader,\n",
        "            epochs=epochs,\n",
        "            lr=lr,\n",
        "            weight_decay=weight_decay,\n",
        "            device=device,\n",
        "        )\n",
        "\n",
        "        val_acc = _accuracy(model, X_val, y_val, device=device)\n",
        "        if (index + 1) % 5 == 0:\n",
        "            print(f\"Val Accuracy after query {index+1}: {val_acc:0.4f}\")\n",
        "        perf_hist.append(val_acc)\n",
        "\n",
        "        X_init, y_init = X_train, y_train\n",
        "\n",
        "    final_test_acc = _accuracy(model, X_test, y_test, device=device)\n",
        "    print(f\"********** Test Accuracy per experiment: {final_test_acc:.4f} **********\")\n",
        "    return perf_hist, final_test_acc\n",
        "\n",
        "\n",
        "def train_active_learning(args, device, datasets):\n",
        "    acq_functions = select_acq_function(args.acq_func)\n",
        "    results = {}\n",
        "    state_loop = [True] if not args.determ else [True, False]\n",
        "\n",
        "    for state in state_loop:\n",
        "        for acq_func in acq_functions:\n",
        "            avg_hist = []\n",
        "            test_scores = []\n",
        "            acq_func_name = f\"{acq_func.__name__}-MC_dropout={state}\"\n",
        "            print(f\"\\n---------- Start {acq_func_name} training! ----------\")\n",
        "            for e in range(args.experiments):\n",
        "                set_seed(args.seed + e)  # Different seed per experiment\n",
        "                print(f\"********** Experiment Iterations: {e + 1}/{args.experiments} **********\")\n",
        "                training_hist, test_score = active_learning_procedure(\n",
        "                    query_strategy=acq_func,\n",
        "                    X_val=datasets[\"X_val\"],\n",
        "                    y_val=datasets[\"y_val\"],\n",
        "                    X_test=datasets[\"X_test\"],\n",
        "                    y_test=datasets[\"y_test\"],\n",
        "                    X_pool=datasets[\"X_pool\"],\n",
        "                    y_pool=datasets[\"y_pool\"],\n",
        "                    X_init=datasets[\"X_init\"],\n",
        "                    y_init=datasets[\"y_init\"],\n",
        "                    build_model=ConvNN,\n",
        "                    T=args.dropout_iter,\n",
        "                    n_query=args.query,\n",
        "                    training=state,\n",
        "                    batch_size=args.batch_size,\n",
        "                    epochs=args.epochs,\n",
        "                    lr=args.lr,\n",
        "                    weight_decay=args.weight_decay,\n",
        "                    device=device,\n",
        "                )\n",
        "                avg_hist.append(training_hist)\n",
        "                test_scores.append(test_score)\n",
        "            avg_hist_arr = np.average(np.array(avg_hist), axis=0)\n",
        "            avg_test = sum(test_scores) / len(test_scores)\n",
        "            print(f\"Average Test score for {acq_func_name}: {avg_test}\")\n",
        "            results[acq_func_name] = avg_hist_arr\n",
        "    return results\n",
        "\n",
        "\n",
        "# Acquisition function codes:\n",
        "# 0: all (uniform, max_entropy, bald, var_ratios, mean_std)\n",
        "# 1: uniform only\n",
        "# 2: max_entropy only\n",
        "# 3: bald only\n",
        "# 4: var_ratios only\n",
        "# 5: mean_std only\n",
        "\n",
        "args = argparse.Namespace(\n",
        "    batch_size=128,\n",
        "    epochs=50,\n",
        "    lr=1e-3,\n",
        "    weight_decay=1e-2,\n",
        "    seed=369,\n",
        "    experiments=1,\n",
        "    dropout_iter=50,\n",
        "    query=10,\n",
        "    acq_func=3,   # all acquisition functions\n",
        "    val_size=100,\n",
        "    determ=True,  # Run both deterministic and Bayesian\n",
        "    result_dir=\"result_npy_exp2\",\n",
        ")\n",
        "\n",
        "set_seed(args.seed)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "data_loader = LoadData(val_size=args.val_size, seed=args.seed)\n",
        "(X_init, y_init, X_val, y_val, X_pool, y_pool, X_test, y_test) = data_loader.load_all()\n",
        "\n",
        "datasets = {\n",
        "    \"X_init\": X_init,\n",
        "    \"y_init\": y_init,\n",
        "    \"X_val\": X_val,\n",
        "    \"y_val\": y_val,\n",
        "    \"X_pool\": X_pool,\n",
        "    \"y_pool\": y_pool,\n",
        "    \"X_test\": X_test,\n",
        "    \"y_test\": y_test,\n",
        "}\n",
        "\n",
        "results = train_active_learning(args, device, datasets)\n",
        "results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8f9c8bb3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "8f9c8bb3",
        "outputId": "2f405246-e351-4181-f41b-8a59e8bdaeee"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "friendly_labels = {\n",
        "    \"uniform-MC_dropout=True\": \"Uniform (Bayesian)\",\n",
        "    \"uniform-MC_dropout=False\": \"Uniform (Deterministic)\",\n",
        "    \"max_entropy-MC_dropout=True\": \"Max Entropy (Bayesian)\",\n",
        "    \"max_entropy-MC_dropout=False\": \"Max Entropy (Deterministic)\",\n",
        "    \"bald-MC_dropout=True\": \"BALD (Bayesian)\",\n",
        "    \"bald-MC_dropout=False\": \"BALD (Deterministic)\",\n",
        "    \"var_ratios-MC_dropout=True\": \"Variation Ratios (Bayesian)\",\n",
        "    \"var_ratios-MC_dropout=False\": \"Variation Ratios (Deterministic)\",\n",
        "    \"mean_std-MC_dropout=True\": \"Mean STD (Bayesian)\",\n",
        "    \"mean_std-MC_dropout=False\": \"Mean STD (Deterministic)\",\n",
        "}\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "for name, curve in results.items():\n",
        "    label = friendly_labels.get(name, name)\n",
        "    is_det = \"Deterministic\" in label\n",
        "    color = \"red\" if is_det else \"blue\"\n",
        "    linestyle = \"-\" if is_det else \"--\"\n",
        "    plt.plot(curve, label=label, linestyle=linestyle, color=color)\n",
        "plt.xlabel(\"Acquisition step\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Active learning accuracy vs queries (Experiment 2)\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
