{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "24538564",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "24538564",
        "outputId": "02e44d32-9be1-4728-df2d-6cd418873da4"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import argparse\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
        "\n",
        "from abc import ABC, abstractmethod\n",
        "from typing import Optional, Tuple\n",
        "\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import MNIST\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "def set_seed(seed: int) -> None:\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "\n",
        "class ConvNN(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_filters: int = 32,\n",
        "        kernel_size: int = 4,\n",
        "        dense_layer: int = 128,\n",
        "        img_rows: int = 28,\n",
        "        img_cols: int = 28,\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(1, num_filters, kernel_size, stride=1)\n",
        "        self.conv2 = nn.Conv2d(num_filters, num_filters, kernel_size, stride=1)\n",
        "        self.dropout1 = nn.Dropout(0.25)\n",
        "        self.dropout2 = nn.Dropout(0.5)\n",
        "        pooled_h = (img_rows - 2 * kernel_size + 2) // 2\n",
        "        pooled_w = (img_cols - 2 * kernel_size + 2) // 2\n",
        "        self.fc1 = nn.Linear(num_filters * pooled_h * pooled_w, dense_layer)\n",
        "        self.fc2 = nn.Linear(dense_layer, 10)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.max_pool2d(x, 2)\n",
        "        x = self.dropout1(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout2(x)\n",
        "        return self.fc2(x)\n",
        "\n",
        "\n",
        "class ConvNNBackbone(nn.Module):\n",
        "    def __init__(self,\n",
        "                 num_filters: int = 32,\n",
        "                kernel_size: int = 4,\n",
        "                img_rows: int = 28,\n",
        "                img_cols: int = 28,\n",
        "                dense_layer: int = 128,\n",
        "                use_fc2_features: bool = False) -> None:\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(1, num_filters, kernel_size, stride=1)\n",
        "        self.conv2 = nn.Conv2d(num_filters, num_filters, kernel_size, stride=1)\n",
        "        self.dropout1 = nn.Dropout(0.25)\n",
        "        self.dropout2 = nn.Dropout(0.5)\n",
        "        pooled_h = (img_rows - 2 * kernel_size + 2) // 2\n",
        "        pooled_w = (img_cols - 2 * kernel_size + 2) // 2\n",
        "        self.fc1 = nn.Linear(num_filters * pooled_h * pooled_w, dense_layer)\n",
        "        self.fc2 = nn.Linear(dense_layer, 10)\n",
        "        self.use_fc2_features = use_fc2_features\n",
        "        self.feature_dim = 10 if use_fc2_features else dense_layer\n",
        "\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"Run a forward pass through the model\"\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.max_pool2d(x, 2)\n",
        "        x = self.dropout1(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout2(x)\n",
        "        if self.use_fc2_features:\n",
        "            return self.fc2(x)\n",
        "        else:\n",
        "            return x\n",
        "\n",
        "    def get_fc1_features(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Explicitly get fc1 features (128-dim) regardless of use_fc2_features setting.\"\"\"\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.max_pool2d(x, 2)\n",
        "        x = self.dropout1(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        return x\n",
        "\n",
        "    def get_logits(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Explicitly get full network output (10-dim logits).\"\"\"\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.max_pool2d(x, 2)\n",
        "        x = self.dropout1(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout2(x)\n",
        "        return self.fc2(x)\n",
        "\n",
        "    def freeze(self) -> None:\n",
        "        for p in self.parameters():\n",
        "            p.requires_grad = False\n",
        "\n",
        "    def unfreeze(self) -> None:\n",
        "        for p in self.parameters():\n",
        "            p.requires_grad = True\n",
        "\n",
        "    def freeze_except_fc2(self) -> None:\n",
        "        \"\"\"Freeze everything except fc2 layer\"\"\"\n",
        "        for name, p in self.named_parameters():\n",
        "            if 'fc2' not in name:\n",
        "                p.requires_grad = False\n",
        "            else:\n",
        "                p.requires_grad = True\n",
        "\n",
        "class HierarchicalBayesModel(nn.Module):\n",
        "    \"\"\"Base class for hierarchical parametrized basis function regression models.\"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        feature_extractor: Optional[ConvNNBackbone],\n",
        "        num_filters: int = 32,\n",
        "        kernel_size: int = 4,\n",
        "        dense_layer: int = 128,\n",
        "        img_rows: int = 28,\n",
        "        img_cols: int = 28,\n",
        "        num_outputs: int = 10,\n",
        "        use_fc2: bool = False,\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "        if feature_extractor is not None:\n",
        "            self.feature_extractor = feature_extractor\n",
        "        else:\n",
        "            self.feature_extractor = ConvNNBackbone(\n",
        "                num_filters=num_filters,\n",
        "                kernel_size=kernel_size,\n",
        "                dense_layer=dense_layer,\n",
        "                img_rows=img_rows,\n",
        "                img_cols=img_cols,\n",
        "                use_fc2_features=use_fc2\n",
        "            )\n",
        "        self.num_outputs = num_outputs\n",
        "        self.use_fc2 = use_fc2\n",
        "\n",
        "    def get_basis_func(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Get the basis function outputs (features) for input x.\"\"\"\n",
        "        if self.use_fc2:\n",
        "            return self.feature_extractor(x)\n",
        "        else:\n",
        "            return self.feature_extractor.get_fc1_features(x)\n",
        "\n",
        "    def freeze_basis_functions(self) -> None:\n",
        "        self.feature_extractor.freeze()\n",
        "\n",
        "    def unfreeze_basis_functions(self) -> None:\n",
        "        self.feature_extractor.unfreeze()\n",
        "\n",
        "    @abstractmethod\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Forward pass returning predictions. Must be implemented by subclasses.\"\"\"\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def predict_with_uncertainty(\n",
        "        self, x: torch.Tensor\n",
        ") -> tuple[torch.Tensor, torch.Tensor]:\n",
        "        \"\"\"Return predictions and predictive variances.\"\"\"\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def compute_predictive_variance(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Computes predictive variance of given data and returns it.\"\"\"\n",
        "        pass\n",
        "\n",
        "\n",
        "\n",
        "class Analytical_HB(HierarchicalBayesModel):\n",
        "    \"\"\"Analytic Inference for Hierarchical Bayes Model\"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        feature_extractor: Optional[ConvNNBackbone],\n",
        "        num_filters: int = 32,\n",
        "        kernel_size: int = 4,\n",
        "        dense_layer: int = 128,\n",
        "        img_rows: int = 28,\n",
        "        img_cols: int = 28,\n",
        "        num_outputs: int = 10,\n",
        "        likelihood_variance: float = 1.0,\n",
        "        prior_variance: float = 1.0,\n",
        "        jitter: float = 1e-6,\n",
        "    ) -> None:\n",
        "        super().__init__(\n",
        "            feature_extractor=feature_extractor,\n",
        "            num_filters=num_filters,\n",
        "            kernel_size=kernel_size,\n",
        "            dense_layer=dense_layer,\n",
        "            img_rows=img_rows,\n",
        "            img_cols=img_cols,\n",
        "            num_outputs=num_outputs,\n",
        "        )\n",
        "        self.likelihood_variance = likelihood_variance\n",
        "        self.prior_variance = prior_variance\n",
        "        self.jitter = jitter\n",
        "        self.posterior_cov: Optional[torch.Tensor] = None\n",
        "        self.posterior_mean: Optional[torch.Tensor] = None\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Predictive mean using current posterior (falls back to raw basis if posterior absent).\"\"\"\n",
        "        phi = self.get_basis_func(x)\n",
        "        if self.posterior_mean is None:\n",
        "            return phi\n",
        "        return phi @ self.posterior_mean\n",
        "\n",
        "    def predict_with_uncertainty(\n",
        "        self, x: torch.Tensor, include_likelihood_noise: bool = True\n",
        ") -> tuple[torch.Tensor, torch.Tensor]:\n",
        "        \"\"\"Predictive mean and variance using closed-form posterior.\"\"\"\n",
        "        phi = self.get_basis_func(x)\n",
        "        if self.posterior_mean is None or self.posterior_cov is None:\n",
        "            base_var = torch.zeros(phi.size(0), device=phi.device, dtype=phi.dtype)\n",
        "            if include_likelihood_noise:\n",
        "                base_var = base_var + self.likelihood_variance\n",
        "            pred_var = base_var.unsqueeze(1).expand(-1, self.num_outputs)\n",
        "            return phi, pred_var\n",
        "        pred_mean = phi @ self.posterior_mean\n",
        "        phi_S = phi @ self.posterior_cov\n",
        "        epistemic = torch.sum(phi_S * phi, dim=1)\n",
        "        if include_likelihood_noise:\n",
        "            total_var = epistemic + self.likelihood_variance\n",
        "        else:\n",
        "            total_var = epistemic\n",
        "        pred_var = total_var.unsqueeze(1).expand(-1, self.num_outputs)\n",
        "        return pred_mean, pred_var\n",
        "\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def compute_posterior(self, x: torch.Tensor, y: torch.Tensor) -> None:\n",
        "        \"\"\"Compute closed-form posterior given design matrix phi (n,k) and targets y (n,d).\"\"\"\n",
        "        sigma_inv = 1.0 / self.likelihood_variance\n",
        "        prior_inv = 1.0 / self.prior_variance\n",
        "        phi = self.get_basis_func(x)\n",
        "        phiT_phi = phi.T @ phi\n",
        "        eye = torch.eye(phi.shape[1], device=phi.device, dtype=phi.dtype)\n",
        "        s_inv = sigma_inv * phiT_phi + prior_inv * eye\n",
        "        s_inv = s_inv + self.jitter * eye\n",
        "        s_cov = torch.linalg.inv(s_inv)\n",
        "        phiT_y = phi.T @ y\n",
        "        mu = s_cov @ (sigma_inv * phiT_y)\n",
        "        self.posterior_cov = s_cov\n",
        "        self.posterior_mean = mu\n",
        "\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def compute_posterior_from_loader(\n",
        "        self, loader: torch.utils.data.DataLoader, device: torch.device\n",
        ") -> None:\n",
        "        \"\"\"Accumulate sufficient statistics over a dataloader and compute posterior.\"\"\"\n",
        "        phiT_phi_accum = None\n",
        "        phiT_y_accum = None\n",
        "        n_total = 0\n",
        "        for batch_x, batch_y in loader:\n",
        "            batch_x = batch_x.to(device)\n",
        "            batch_y = batch_y.to(device)\n",
        "            phi = self.get_basis_func(batch_x)\n",
        "            if phiT_phi_accum is None:\n",
        "                k = phi.shape[1]\n",
        "                d = batch_y.shape[1]\n",
        "                phiT_phi_accum = torch.zeros((k, k), device=device, dtype=phi.dtype)\n",
        "                phiT_y_accum = torch.zeros((k, d), device=device, dtype=phi.dtype)\n",
        "            phiT_phi_accum = phiT_phi_accum + phi.T @ phi\n",
        "            phiT_y_accum = phiT_y_accum + phi.T @ batch_y\n",
        "            n_total += batch_x.size(0)\n",
        "        assert phiT_y_accum is not None\n",
        "        sigma_inv = 1.0 / self.likelihood_variance\n",
        "        prior_inv = 1.0 / self.prior_variance\n",
        "        eye = torch.eye(k, device=device, dtype=phiT_phi_accum.dtype)\n",
        "        s_inv = sigma_inv * phiT_phi_accum + prior_inv * eye\n",
        "        s_inv = s_inv + self.jitter * eye\n",
        "        s_cov = torch.linalg.inv(s_inv)\n",
        "        mu = s_cov @ (sigma_inv * phiT_y_accum)\n",
        "        self.posterior_cov = s_cov\n",
        "        self.posterior_mean = mu\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def compute_predictive_variance(self, X: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Compute predictive variance for each sample in X.\"\"\"\n",
        "        dev = next(self.feature_extractor.parameters()).device\n",
        "        x_tensor = torch.from_numpy(X).float().to(dev)\n",
        "        phi = self.get_basis_func(x_tensor)\n",
        "        if self.posterior_mean is None or self.posterior_cov is None:\n",
        "            variances = np.zeros(phi.size(0))\n",
        "            return variances\n",
        "        var_w = self.posterior_cov\n",
        "        phi_S = phi @ var_w\n",
        "        epistemic_var = torch.sum(phi_S * phi, dim=1)\n",
        "        total_var = epistemic_var + self.likelihood_variance\n",
        "        return total_var.detach().cpu().numpy()\n",
        "\n",
        "\n",
        "class MFVI_HB(HierarchicalBayesModel):\n",
        "    \"\"\"Mean-field variational inference (MFVI) for Hierarchical Bayes Model\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        feature_extractor: Optional[ConvNNBackbone],\n",
        "        num_filters: int = 32,\n",
        "        kernel_size: int = 4,\n",
        "        dense_layer: int = 128,\n",
        "        img_rows: int = 28,\n",
        "        img_cols: int = 28,\n",
        "        num_outputs: int = 10,\n",
        "        likelihood_variance: float = 1.0,\n",
        "        prior_variance: float = 1.0,\n",
        "        jitter: float = 1e-6,\n",
        "        elbo_lr: float = 1e-3,\n",
        "        vi_method: str ='closed',\n",
        "        **kwargs\n",
        "    ) -> None:\n",
        "        super().__init__(\n",
        "            feature_extractor=feature_extractor,\n",
        "            num_filters=num_filters,\n",
        "            kernel_size=kernel_size,\n",
        "            dense_layer=dense_layer,\n",
        "            img_rows=img_rows,\n",
        "            img_cols=img_cols,\n",
        "            num_outputs=num_outputs,\n",
        "        )\n",
        "        self.likelihood_variance = likelihood_variance\n",
        "        self.prior_variance = prior_variance\n",
        "        self.jitter = jitter\n",
        "        self.elbo_lr = elbo_lr\n",
        "        self.vi_method = vi_method\n",
        "        self.posterior_cov: Optional[torch.Tensor] = None\n",
        "        self.posterior_mean: Optional[torch.Tensor] = None\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Predictive mean using current posterior\"\"\"\n",
        "        phi = self.get_basis_func(x)\n",
        "        if self.posterior_mean is None:\n",
        "            return phi\n",
        "        return phi @ self.posterior_mean\n",
        "\n",
        "    def _compute_A_k(self, phi: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Computes A_k = \\\\sum_n phi_nk^2 for each feature k.\"\"\"\n",
        "        return torch.sum(phi ** 2, dim=0)\n",
        "\n",
        "    def _update_M_closed_form(self, phi: torch.Tensor, y: torch.Tensor, sigma2: float) -> torch.Tensor:\n",
        "        \"\"\"Solve closed-form ridge regression for posterior mean M.\"\"\"\n",
        "        k = phi.shape[1]\n",
        "        device = phi.device\n",
        "        dtype = phi.dtype\n",
        "        phiT_phi = phi.T @ phi\n",
        "        phiT_y = phi.T @ y\n",
        "        eye = torch.eye(k, device=device, dtype=dtype)\n",
        "        lambda_reg = (sigma2 / self.prior_variance) * eye\n",
        "        M = torch.linalg.solve(phiT_phi + lambda_reg, phiT_y)\n",
        "        return M\n",
        "\n",
        "    def _update_variances_closed_form(self, A_k: torch.Tensor, sigma2: float) -> torch.Tensor:\n",
        "        \"\"\"Compute posterior variances sigma_kd² using closed form.\n",
        "        sigma_kd^2 = 1 / (A_k / \\\\sigma^2 + 1 / s^2)\n",
        "        Note: sigma_kd^2 depends on k but not on d (shared variance across outputs for each feature).\n",
        "        \"\"\"\n",
        "        inv_term = (A_k / sigma2) + (1.0 / self.prior_variance)\n",
        "        var_diag = 1.0 / inv_term\n",
        "        S = torch.diag(var_diag)\n",
        "        return S\n",
        "\n",
        "    def _update_sigma2(self, y: torch.Tensor, phi: torch.Tensor, M: torch.Tensor, S: torch.Tensor) -> float:\n",
        "        \"\"\"Compute ML estimate of observation noise variance sigma^2.\n",
        "        sigma^2* = (1/ND) * (Res + V)\n",
        "        where Res = ||Y - phi M||_F^2, V = \\\\sum_n phi_n^T diag(SS^T) phi_n\n",
        "        Returns:\n",
        "            sigma2: updated likelihood variance\n",
        "        \"\"\"\n",
        "        n, d = y.shape\n",
        "        residuals = y - phi @ M\n",
        "        Res = torch.sum(residuals ** 2)\n",
        "        S_diag = torch.diag(S)\n",
        "        A_k = self._compute_A_k(phi)\n",
        "        V = torch.sum(A_k * S_diag)\n",
        "\n",
        "        sigma2_new = float((Res + V) / (n * d))\n",
        "        return max(sigma2_new, 1e-8)\n",
        "\n",
        "    def _compute_posterior_coord_ascent(self, phi: torch.Tensor, y: torch.Tensor, max_iters: int=20) -> None:\n",
        "        \"\"\"Coordinate ascent optimization for MFVI posterior.\"\"\"\n",
        "        max_iterations = max_iters\n",
        "        tolerance = 1e-4\n",
        "        sigma2 = self.likelihood_variance\n",
        "        for iter in range(max_iterations):\n",
        "            M = self._update_M_closed_form(phi, y, sigma2)\n",
        "            A_k = self._compute_A_k(phi)\n",
        "            S = self._update_variances_closed_form(A_k, sigma2)\n",
        "            sigma2_new = self._update_sigma2(y, phi, M, S)\n",
        "            sigma2_change = abs(sigma2_new - sigma2)\n",
        "            if sigma2_change < tolerance:\n",
        "                break\n",
        "            sigma2 = sigma2_new\n",
        "\n",
        "        self.posterior_mean = M.detach()\n",
        "        self.posterior_cov = S.detach()\n",
        "\n",
        "    def predict_with_uncertainty(self, x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n",
        "        phi = self.get_basis_func(x)\n",
        "        if self.posterior_mean is None or self.posterior_cov is None:\n",
        "            base_var = torch.zeros(phi.size(0), device=phi.device, dtype=phi.dtype)\n",
        "            base_var = base_var + self.likelihood_variance\n",
        "            pred_var = base_var.unsqueeze(1).expand(-1, self.num_outputs)\n",
        "            return phi, pred_var\n",
        "        predictive_mean = phi @ self.posterior_mean\n",
        "        var_w = self.posterior_cov\n",
        "        phi_S = phi @ var_w\n",
        "        epistemic_var = torch.sum(phi_S * phi, dim=1)\n",
        "        per_dim_var = self.likelihood_variance + epistemic_var\n",
        "        predictive_var = per_dim_var.unsqueeze(1).expand(-1, self.num_outputs)\n",
        "        return predictive_mean, predictive_var\n",
        "\n",
        "    def compute_posterior(self, x: torch.Tensor, y: torch.Tensor) -> None:\n",
        "        \"\"\"Compute posterior for MFVI head with configurable VI method.\n",
        "\n",
        "        Routes based on `self.vi_method`:\n",
        "        - \"closed\": closed-form M,S with fixed σ² = likelihood_variance\n",
        "        - \"coord\": coordinate-ascent updating M,S,σ² until convergence\n",
        "        - fallback: original ELBO gradient optimization\n",
        "        \"\"\"\n",
        "        phi = self.get_basis_func(x)\n",
        "\n",
        "        if self.vi_method == \"closed\":\n",
        "            sigma2 = self.likelihood_variance\n",
        "            M = self._update_M_closed_form(phi, y, sigma2)\n",
        "            A_k = self._compute_A_k(phi)\n",
        "            S = self._update_variances_closed_form(A_k, sigma2)\n",
        "            self.posterior_mean = M.detach()\n",
        "            self.posterior_cov = S.detach()\n",
        "            return\n",
        "\n",
        "        if self.vi_method == \"coord\":\n",
        "            self._compute_posterior_coord_ascent(phi, y)\n",
        "            return\n",
        "\n",
        "        print(\"ELBO calculation failed - falling back to gradient descent\")\n",
        "        k = phi.shape[1]\n",
        "        d = y.shape[1]\n",
        "        device = phi.device\n",
        "        dtype = phi.dtype\n",
        "\n",
        "        mu = torch.zeros((k, d), device=device, dtype=dtype, requires_grad=True)\n",
        "        log_var_diag = torch.zeros((k,), device=device, dtype=dtype, requires_grad=True)\n",
        "\n",
        "        optimizer = torch.optim.Adam([mu, log_var_diag], lr=self.elbo_lr)\n",
        "        sigma_inv = 1.0 / self.likelihood_variance\n",
        "        prior_inv = 1.0 / self.prior_variance\n",
        "\n",
        "        for _ in range(50):\n",
        "            var_diag = torch.exp(log_var_diag)\n",
        "            eps = torch.randn((k, d), device=device, dtype=dtype)\n",
        "            w_sample = mu + torch.sqrt(var_diag).unsqueeze(1) * eps\n",
        "\n",
        "            y_pred = phi @ w_sample\n",
        "            residuals = y - y_pred\n",
        "\n",
        "            norm_const = -0.5 * d * phi.size(0) * np.log(2 * np.pi * self.likelihood_variance)\n",
        "            mse_term = -0.5 * sigma_inv * torch.sum(residuals ** 2)\n",
        "            var_contrib = torch.sum((phi @ torch.diag(var_diag)) * phi)\n",
        "            trace_term = -0.5 * sigma_inv * var_contrib\n",
        "            exp_log_lik = norm_const + mse_term + trace_term\n",
        "\n",
        "            kl_var = 0.5 * d * torch.sum(-torch.log(var_diag + 1e-8) - 1.0 + prior_inv * var_diag)\n",
        "            kl_mu = 0.5 * prior_inv * torch.sum(mu ** 2)\n",
        "            kl_div = kl_var + kl_mu\n",
        "\n",
        "            elbo = exp_log_lik - kl_div\n",
        "            loss = -elbo / phi.size(0)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        self.posterior_mean = mu.detach()\n",
        "        self.posterior_cov = torch.diag(torch.exp(log_var_diag.detach()))\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def compute_predictive_variance(self, X: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Compute predictive variance for each sample in X.\"\"\"\n",
        "        dev = next(self.feature_extractor.parameters()).device\n",
        "        x_tensor = torch.from_numpy(X).float().to(dev)\n",
        "        phi = self.get_basis_func(x_tensor)\n",
        "        if self.posterior_mean is None or self.posterior_cov is None:\n",
        "            variances = np.zeros(phi.size(0))\n",
        "            return variances\n",
        "        var_w = self.posterior_cov\n",
        "        phi_S = phi @ var_w\n",
        "        epistemic_var = torch.sum(phi_S * phi, dim=1)\n",
        "        total_var = epistemic_var + self.likelihood_variance\n",
        "\n",
        "        return total_var.detach().cpu().numpy()\n",
        "\n",
        "\n",
        "def get_balanced_initial_set(\n",
        "    X: np.ndarray,\n",
        "    y: np.ndarray,\n",
        "    samples_per_class: int = 2,\n",
        "    num_classes: int = 10,\n",
        "):\n",
        "    \"\"\"Extract balanced initial set: samples_per_class per class; return updated pool.\"\"\"\n",
        "    init_idx = []\n",
        "    for cls in range(num_classes):\n",
        "        cls_indices = np.where(y == cls)[0]\n",
        "        if len(cls_indices) < samples_per_class:\n",
        "            raise ValueError(\n",
        "                f\"Not enough samples for class {cls}: {len(cls_indices)} available, need {samples_per_class}\"\n",
        "            )\n",
        "        sampled = np.random.choice(cls_indices, size=samples_per_class, replace=False)\n",
        "        init_idx.extend(sampled)\n",
        "    init_idx = np.array(init_idx)\n",
        "    pool_mask = np.ones(len(X), dtype=bool)\n",
        "    pool_mask[init_idx] = False\n",
        "    pool_idx = np.where(pool_mask)[0]\n",
        "\n",
        "    return X[init_idx], y[init_idx], X[pool_idx], y[pool_idx]\n",
        "\n",
        "class LoadData:\n",
        "    \"\"\"Download, split, and prepare MNIST for active learning.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        seed: int = 271,\n",
        "        pretrain_size: int = 1000,\n",
        "        val_size: int = 100,\n",
        "        train_size: int = 20,\n",
        "        root: str = \"data\",\n",
        "        initial_per_class: int = 2,\n",
        "    ) -> None:\n",
        "        self.seed = seed\n",
        "        self.pretrain_size = pretrain_size\n",
        "        self.train_size = train_size\n",
        "        self.val_size = val_size\n",
        "        self.root = root\n",
        "        self.initial_per_class = initial_per_class\n",
        "        self.mnist_train, self.mnist_test = self.download_dataset()\n",
        "        self.pool_size = len(self.mnist_train) - self.train_size - self.val_size - self.pretrain_size\n",
        "        (\n",
        "            self.X_pretrain_All,\n",
        "            self.y_pretrain_All,\n",
        "            self.X_train_All,\n",
        "            self.y_train_All,\n",
        "            self.X_val,\n",
        "            self.y_val,\n",
        "            self.X_pool,\n",
        "            self.y_pool,\n",
        "            self.X_test,\n",
        "            self.y_test,\n",
        "        ) = self.split_and_load_dataset()\n",
        "        self.X_init, self.y_init = self.preprocess_training_data()\n",
        "\n",
        "    def tensor_to_np(self, tensor_data: torch.Tensor) -> np.ndarray:\n",
        "        return tensor_data.detach().cpu().numpy()\n",
        "\n",
        "    def check_mnist_folder(self) -> bool:\n",
        "        return not os.path.exists(os.path.join(self.root, \"MNIST\"))\n",
        "\n",
        "    def download_dataset(self):\n",
        "        transform = transforms.Compose(\n",
        "            [transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]\n",
        "        )\n",
        "        download = self.check_mnist_folder()\n",
        "        mnist_train = MNIST(\n",
        "            self.root, train=True, download=download, transform=transform\n",
        "        )\n",
        "        mnist_test = MNIST(self.root, train=False, download=download, transform=transform)\n",
        "        return mnist_train, mnist_test\n",
        "\n",
        "    def split_and_load_dataset(self):\n",
        "        generator = torch.Generator().manual_seed(self.seed)\n",
        "        pretrain_set, train_set, val_set, pool_set = random_split(\n",
        "            self.mnist_train,\n",
        "            [self.pretrain_size, self.train_size, self.val_size, self.pool_size],\n",
        "            generator=generator,\n",
        "        )\n",
        "        pretrain_loader = DataLoader(\n",
        "            dataset = pretrain_set, batch_size=self.pretrain_size,shuffle=True\n",
        "        )\n",
        "        train_loader = DataLoader(\n",
        "            dataset=train_set, batch_size=self.train_size, shuffle=True\n",
        "        )\n",
        "        val_loader = DataLoader(dataset=val_set, batch_size=self.val_size, shuffle=True)\n",
        "        pool_loader = DataLoader(\n",
        "            dataset=pool_set, batch_size=self.pool_size, shuffle=True\n",
        "        )\n",
        "        test_loader = DataLoader(dataset=self.mnist_test, batch_size=10000, shuffle=True)\n",
        "        X_pretrain_All, y_pretrain_All = next(iter(pretrain_loader))\n",
        "        X_train_All, y_train_All = next(iter(train_loader))\n",
        "        X_val, y_val = next(iter(val_loader))\n",
        "        X_pool, y_pool = next(iter(pool_loader))\n",
        "        X_test, y_test = next(iter(test_loader))\n",
        "        return X_pretrain_All, y_pretrain_All, X_train_All, y_train_All, X_val, y_val, X_pool, y_pool, X_test, y_test\n",
        "\n",
        "\n",
        "    def preprocess_training_data(self):\n",
        "        \"\"\"Build a balanced initial set with equal samples per class using the helper.\"\"\"\n",
        "        per_class = self.initial_per_class\n",
        "        X_concat = torch.cat([self.X_train_All, self.X_pool], dim=0).detach().cpu().numpy()\n",
        "        y_concat = torch.cat([self.y_train_All, self.y_pool], dim=0).detach().cpu().numpy()\n",
        "        X_init_np, y_init_np, X_pool_np, y_pool_np = get_balanced_initial_set(\n",
        "            X_concat, y_concat, samples_per_class=per_class, num_classes=10,\n",
        "        )\n",
        "        X_init = torch.from_numpy(X_init_np).float()\n",
        "        y_init = torch.from_numpy(y_init_np).long()\n",
        "        self.X_pool = torch.from_numpy(X_pool_np).float()\n",
        "        self.y_pool = torch.from_numpy(y_pool_np).long()\n",
        "\n",
        "        print(f\"Initial training data points: {X_init.shape[0]}\")\n",
        "        binc = np.bincount(y_init.detach().cpu().numpy(), minlength=10)\n",
        "        print(f\"Data distribution for each class: {binc}\")\n",
        "        return X_init, y_init\n",
        "\n",
        "\n",
        "    def load_all(self):\n",
        "        return (\n",
        "            self.tensor_to_np(self.X_pretrain_All),\n",
        "            self.tensor_to_np(self.y_pretrain_All),\n",
        "            self.tensor_to_np(self.X_init),\n",
        "            self.tensor_to_np(self.y_init),\n",
        "            self.tensor_to_np(self.X_val),\n",
        "            self.tensor_to_np(self.y_val),\n",
        "            self.tensor_to_np(self.X_pool),\n",
        "            self.tensor_to_np(self.y_pool),\n",
        "            self.tensor_to_np(self.X_test),\n",
        "            self.tensor_to_np(self.y_test),\n",
        "        )\n",
        "\n",
        "\n",
        "def uniform_acq(model: nn.Module, X_pool: np.ndarray, n_query: int = 10, **_):\n",
        "    n_query = min(n_query, len(X_pool))\n",
        "    query_idx = np.random.choice(range(len(X_pool)), size=n_query, replace=False)\n",
        "    return query_idx, X_pool[query_idx]\n",
        "\n",
        "def predictive_variance_acq(model: nn.Module, X_pool: np.ndarray, n_query: int= 10, **_):\n",
        "    n_query = min(n_query, len(X_pool))\n",
        "    variances = model.compute_predictive_variance(X_pool)\n",
        "    query_idx = np.argsort(-variances)[:n_query]\n",
        "    return query_idx, X_pool[query_idx]\n",
        "\n",
        "def _make_loader(X: np.ndarray, y: np.ndarray, batch_size: int, shuffle: bool = True):\n",
        "    dataset = TensorDataset(torch.from_numpy(X).float(), torch.from_numpy(y).long())\n",
        "    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
        "\n",
        "def _accuracy(model: nn.Module, X: np.ndarray, y: np.ndarray, device: torch.device) -> float:\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        xb = torch.from_numpy(X).float().to(device)\n",
        "        yb = torch.from_numpy(y).long().to(device)\n",
        "        preds = torch.argmax(model(xb), dim=1)\n",
        "        return float((preds == yb).float().mean().cpu().item())\n",
        "\n",
        "def one_hot_labels(y_labels: np.ndarray, num_classes: int = 10) -> np.ndarray:\n",
        "    \"\"\"Convert integer class labels to one-hot encoded array.\"\"\"\n",
        "    y_labels = y_labels.astype(int)\n",
        "    y_one_hot = np.zeros((len(y_labels), num_classes), dtype=np.float32)\n",
        "    y_one_hot[np.arange(len(y_labels)), y_labels] = 1.0\n",
        "    return y_one_hot\n",
        "\n",
        "def _mse_rmse(model: nn.Module, X: np.ndarray, y: np.ndarray, device: torch.device) -> tuple[float, float]:\n",
        "    \"\"\"Compute MSE and RMSE on raw logits vs one-hot targets.\"\"\"\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        xb = torch.from_numpy(X).float().to(device)\n",
        "        y_one_hot = torch.from_numpy(one_hot_labels(y.astype(int), num_classes=10)).float().to(device)\n",
        "        logits = model(xb)\n",
        "        mse = torch.mean((logits - y_one_hot) ** 2).item()\n",
        "        rmse = float(np.sqrt(mse))\n",
        "        return mse, rmse\n",
        "\n",
        "def pretrainer(\n",
        "        backbone_model: ConvNNBackbone, X_pretrain: np.ndarray, Y_pretrain: np.ndarray,\n",
        "               epochs: int, lr: float, weight_decay: float, device: torch.device,\n",
        "                       batch_size: int = 128, task: str = 'classification',\n",
        "                       X_val: np.ndarray | None = None, y_val: np.ndarray | None = None,\n",
        "                       val_batch_size: int = 256) -> Tuple[ConvNNBackbone, float, float, float]:\n",
        "    \"\"\"Train CNN backbone and report train acc plus validation acc/MSE.\"\"\"\n",
        "    opt = torch.optim.Adam(list(backbone_model.parameters()), lr=lr, weight_decay=weight_decay)\n",
        "    if task == 'classification':\n",
        "        crit = nn.CrossEntropyLoss()\n",
        "    else:\n",
        "        crit = nn.MSELoss()\n",
        "    backbone_model.to(device).train()\n",
        "\n",
        "    if task == 'classification':\n",
        "        y_labels = Y_pretrain.astype(int)\n",
        "        loader = _make_loader(X_pretrain, y_labels, batch_size, shuffle=True)\n",
        "    else:\n",
        "        y_labels = Y_pretrain.astype(int)\n",
        "        y_one_hot = np.zeros((len(y_labels), 10), dtype=np.float32)\n",
        "        y_one_hot[np.arange(len(y_labels)), y_labels] = 1.0\n",
        "        dataset = TensorDataset(\n",
        "            torch.from_numpy(X_pretrain).float(),\n",
        "            torch.from_numpy(y_one_hot).float()\n",
        "        )\n",
        "        loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        for xb, yb in loader:\n",
        "            xb = xb.to(device)\n",
        "            yb = yb.to(device)\n",
        "            opt.zero_grad()\n",
        "            preds = backbone_model(xb)\n",
        "            loss = crit(preds, yb)\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "\n",
        "    backbone_model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for xb, yb in loader:\n",
        "            xb = xb.to(device)\n",
        "            preds = backbone_model(xb)\n",
        "            if task == 'classification':\n",
        "                yb_labels = yb.to(device)\n",
        "                pred_labels = torch.argmax(preds, dim=1)\n",
        "                correct += (pred_labels == yb_labels).sum().item()\n",
        "            else:\n",
        "                yb = yb.to(device)\n",
        "                pred_labels = torch.argmax(preds, dim=1)\n",
        "                true_labels = torch.argmax(yb, dim=1)\n",
        "                correct += (pred_labels == true_labels).sum().item()\n",
        "            total += xb.size(0)\n",
        "    pretrain_acc = correct / total\n",
        "    val_acc = float('nan')\n",
        "    val_mse = float('nan')\n",
        "    if X_val is not None and y_val is not None:\n",
        "        if task == 'classification':\n",
        "            y_val_labels = y_val.astype(int)\n",
        "            val_loader = _make_loader(X_val, y_val_labels, val_batch_size, shuffle=False)\n",
        "            y_val_one_hot = np.zeros((len(y_val_labels), 10), dtype=np.float32)\n",
        "            y_val_one_hot[np.arange(len(y_val_labels)), y_val_labels] = 1.0\n",
        "            y_val_one_hot_t = torch.from_numpy(y_val_one_hot).float().to(device)\n",
        "        else:\n",
        "            y_val_labels = y_val.astype(int)\n",
        "            y_val_one_hot = np.zeros((len(y_val_labels), 10), dtype=np.float32)\n",
        "            y_val_one_hot[np.arange(len(y_val_labels)), y_val_labels] = 1.0\n",
        "            dataset = TensorDataset(torch.from_numpy(X_val).float(), torch.from_numpy(y_val_one_hot).float())\n",
        "            val_loader = DataLoader(dataset, batch_size=val_batch_size, shuffle=False)\n",
        "            y_val_one_hot_t = None\n",
        "\n",
        "        correct_val = 0\n",
        "        total_val = 0\n",
        "        mse_accum = 0.0\n",
        "        with torch.no_grad():\n",
        "            for xb, yb in val_loader:\n",
        "                xb = xb.to(device)\n",
        "                if task == 'classification':\n",
        "                    yb_labels = yb.to(device)\n",
        "                    logits = backbone_model(xb)\n",
        "                    pred_labels = torch.argmax(logits, dim=1)\n",
        "                    correct_val += (pred_labels == yb_labels).sum().item()\n",
        "                    if y_val_one_hot_t is not None:\n",
        "                        start = total_val\n",
        "                        end = total_val + xb.size(0)\n",
        "                        target_slice = y_val_one_hot_t[start:end]\n",
        "                        probs = torch.softmax(logits, dim=1)\n",
        "                        mse_accum += torch.sum((probs - target_slice) ** 2).item()\n",
        "                else:\n",
        "                    logits = backbone_model(xb)\n",
        "                    yb = yb.to(device)\n",
        "                    pred_labels = torch.argmax(logits, dim=1)\n",
        "                    true_labels = torch.argmax(yb, dim=1)\n",
        "                    correct_val += (pred_labels == true_labels).sum().item()\n",
        "                    probs = torch.softmax(logits, dim=1)\n",
        "                    mse_accum += torch.sum((probs - yb) ** 2).item()\n",
        "                total_val += xb.size(0)\n",
        "        val_acc = correct_val / total_val if total_val > 0 else float('nan')\n",
        "        val_mse = mse_accum / total_val if total_val > 0 else float('nan')\n",
        "    return backbone_model, pretrain_acc, val_acc, val_mse\n",
        "\n",
        "def init_head_posterior(\n",
        "    model: torch.nn.Module,\n",
        "    X_init: np.ndarray,\n",
        "    y_init: np.ndarray,\n",
        "    device: torch.device,\n",
        "    num_classes: int = 10,\n",
        ") -> tuple[np.ndarray, np.ndarray]:\n",
        "    \"\"\"Initialize Bayesian head posterior on the initial labeled set.\"\"\"\n",
        "    X_labeled = X_init.copy()\n",
        "    y_labeled_oh = one_hot_labels(y_init, num_classes=num_classes)\n",
        "    x_t = torch.from_numpy(X_labeled).float().to(device)\n",
        "    y_t = torch.from_numpy(y_labeled_oh).float().to(device)\n",
        "    model.compute_posterior(x=x_t, y=y_t)\n",
        "    return X_labeled, y_labeled_oh\n",
        "\n",
        "def select_queries(\n",
        "    model: torch.nn.Module,\n",
        "    X_pool_run: np.ndarray,\n",
        "    n_query: int,\n",
        "    acq_fn: str = \"predictive\",\n",
        ") -> np.ndarray:\n",
        "    \"\"\"Select indices from pool according to the acquisition function.\"\"\"\n",
        "    n_query = min(n_query, len(X_pool_run))\n",
        "    if n_query <= 0:\n",
        "        return np.array([], dtype=int)\n",
        "    if acq_fn == \"predictive\":\n",
        "        variances = model.compute_predictive_variance(X_pool_run)\n",
        "        return np.argsort(-variances)[:n_query]\n",
        "    return np.random.choice(range(len(X_pool_run)), size=n_query, replace=False)\n",
        "\n",
        "def update_sets_after_query(\n",
        "    X_labeled: np.ndarray,\n",
        "    y_labeled_oh: np.ndarray,\n",
        "    X_pool_run: np.ndarray,\n",
        "    y_pool_oh_run: np.ndarray,\n",
        "    query_idx: np.ndarray,\n",
        ") -> tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
        "    \"\"\"Append queried samples to labeled set and remove them from pool.\"\"\"\n",
        "    if query_idx.size == 0:\n",
        "        return X_labeled, y_labeled_oh, X_pool_run, y_pool_oh_run\n",
        "    X_labeled = np.concatenate([X_labeled, X_pool_run[query_idx]], axis=0)\n",
        "    y_labeled_oh = np.concatenate([y_labeled_oh, y_pool_oh_run[query_idx]], axis=0)\n",
        "    X_pool_run = np.delete(X_pool_run, query_idx, axis=0)\n",
        "    y_pool_oh_run = np.delete(y_pool_oh_run, query_idx, axis=0)\n",
        "    return X_labeled, y_labeled_oh, X_pool_run, y_pool_oh_run\n",
        "\n",
        "def recompute_posterior(\n",
        "    model: torch.nn.Module,\n",
        "    X_labeled: np.ndarray,\n",
        "    y_labeled_oh: np.ndarray,\n",
        "    device: torch.device,\n",
        ") -> None:\n",
        "    \"\"\"Recompute posterior on the current labeled set.\"\"\"\n",
        "    x_t = torch.from_numpy(X_labeled).float().to(device)\n",
        "    y_t = torch.from_numpy(y_labeled_oh).float().to(device)\n",
        "    model.compute_posterior(x=x_t, y=y_t)\n",
        "\n",
        "def acquisition_round(\n",
        "    model: torch.nn.Module,\n",
        "    X_labeled: np.ndarray,\n",
        "    y_labeled_oh: np.ndarray,\n",
        "    X_pool_run: np.ndarray,\n",
        "    y_pool_oh_run: np.ndarray,\n",
        "    n_query: int,\n",
        "    acq_fn: str,\n",
        "    device: torch.device,\n",
        ") -> tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
        "    \"\"\"Perform one acquisition round: select, update sets, and recompute posterior.\"\"\"\n",
        "    query_idx = select_queries(model, X_pool_run, n_query=n_query, acq_fn=acq_fn)\n",
        "    X_labeled, y_labeled_oh, X_pool_run, y_pool_oh_run = update_sets_after_query(\n",
        "        X_labeled, y_labeled_oh, X_pool_run, y_pool_oh_run, query_idx\n",
        "    )\n",
        "    recompute_posterior(model, X_labeled, y_labeled_oh, device)\n",
        "    return X_labeled, y_labeled_oh, X_pool_run, y_pool_oh_run, query_idx\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "args = argparse.Namespace(\n",
        "    seed=271,\n",
        "    batch_size=64,\n",
        "    pretrain_epochs=100,\n",
        "    acq_rounds=95,\n",
        "    vi_method='closed',                # ['closed','coord']\n",
        "    pretrain_lr=1e-3,\n",
        "    lr=1e-3,\n",
        "    pretrain_weight_decay=1e-2,\n",
        "    weight_decay=1e-2,\n",
        "    query=10,\n",
        "    pretrain_size=200,\n",
        "    val_size=1000,\n",
        "    initial_labeled_per_class=5,       # Number of samples per class in initial labeled set\n",
        "    pretrain_task=\"classification\",    # ['regression', 'classification']\n",
        "    inference_type=\"all\",              # ['mfvi','analytic','all']\n",
        "    acq_fn=\"all\",                      # ['predictive','random','all']\n",
        "    likelihood_variance=1,\n",
        "    prior_variance=1,\n",
        "    result_dir=\"min_extension\",\n",
        ")\n",
        "\n",
        "set_seed(args.seed)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "data_loader = LoadData(\n",
        "    seed=args.seed,\n",
        "    pretrain_size=args.pretrain_size,\n",
        "    val_size=args.val_size,\n",
        "    initial_per_class=args.initial_labeled_per_class\n",
        ")\n",
        "(X_pretrain, y_pretrain, X_init, y_init, X_val, y_val, X_pool, y_pool, X_test, y_test) = data_loader.load_all()\n",
        "\n",
        "datasets = {\n",
        "    \"X_pretrain\":X_pretrain,\n",
        "    \"y_pretrain\":y_pretrain,\n",
        "    \"X_init\": X_init,\n",
        "    \"y_init\": y_init,\n",
        "    \"X_val\": X_val,\n",
        "    \"y_val\": y_val,\n",
        "    \"X_pool\": X_pool,\n",
        "    \"y_pool\": y_pool,\n",
        "    \"X_test\": X_test,\n",
        "    \"y_test\": y_test,\n",
        "}\n",
        "backbone = ConvNNBackbone(\n",
        "                num_filters=32,\n",
        "                kernel_size=4,\n",
        "                img_rows=28,\n",
        "                img_cols=28,\n",
        "                dense_layer=128,\n",
        "                use_fc2_features=True)\n",
        "backbone, backbone_train_acc, backbone_val_acc, backbone_val_mse = pretrainer(\n",
        "    backbone_model=backbone,\n",
        "    X_pretrain=datasets['X_pretrain'],\n",
        "    Y_pretrain=datasets['y_pretrain'],\n",
        "    epochs=args.pretrain_epochs,\n",
        "    lr=args.pretrain_lr,\n",
        "    weight_decay=args.pretrain_weight_decay,\n",
        "    device=device,\n",
        "    task=args.pretrain_task,\n",
        "    X_val=datasets['X_val'],\n",
        "    y_val=datasets['y_val']\n",
        ")\n",
        "print(f\"Backbone pretraining complete. Validation MSE: {backbone_val_mse:.6f}\")\n",
        "backbone.freeze()\n",
        "models = []\n",
        "acq_fns = []\n",
        "if args.inference_type=='all':\n",
        "    models = ['analytic','mfvi']\n",
        "else:\n",
        "    models.append(args.inference_type)\n",
        "if args.acq_fn == 'all':\n",
        "    acq_fns = ['predictive','random']\n",
        "else:\n",
        "    acq_fns.append(args.acq_fn)\n",
        "\n",
        "results_rmse = {}\n",
        "results_mse = {}\n",
        "for model_type in models:\n",
        "    for acq_fn in acq_fns:\n",
        "        if model_type == 'analytic':\n",
        "            model = Analytical_HB(feature_extractor=backbone,\n",
        "                                likelihood_variance=args.likelihood_variance,\n",
        "                                prior_variance=args.prior_variance)\n",
        "        elif model_type == 'mfvi':\n",
        "            model = MFVI_HB(feature_extractor=backbone,\n",
        "                            vi_method=args.vi_method,\n",
        "                            likelihood_variance=args.likelihood_variance,\n",
        "                            prior_variance=args.prior_variance,\n",
        "            )\n",
        "        X_labeled, y_labeled_oh = init_head_posterior(\n",
        "            model,\n",
        "            datasets[\"X_init\"],\n",
        "            datasets[\"y_init\"],\n",
        "            device,\n",
        "            num_classes=10,\n",
        "        )\n",
        "\n",
        "        X_pool_run = datasets[\"X_pool\"].copy()\n",
        "        y_pool_oh_run = one_hot_labels(datasets[\"y_pool\"].astype(int), num_classes=10)\n",
        "\n",
        "        rmse_curve = []\n",
        "        mse_curve = []\n",
        "        for rnd in range(args.acq_rounds):\n",
        "            X_labeled, y_labeled_oh, X_pool_run, y_pool_oh_run, _ = acquisition_round(\n",
        "                model=model,\n",
        "                X_labeled=X_labeled,\n",
        "                y_labeled_oh=y_labeled_oh,\n",
        "                X_pool_run=X_pool_run,\n",
        "                y_pool_oh_run=y_pool_oh_run,\n",
        "                n_query=args.query,\n",
        "                acq_fn=acq_fn,\n",
        "                device=device,\n",
        "            )\n",
        "            val_mse, val_rmse = _mse_rmse(model, datasets[\"X_val\"], datasets[\"y_val\"], device)\n",
        "            mse_curve.append(val_mse)\n",
        "            rmse_curve.append(val_rmse)\n",
        "\n",
        "            print(f\"Round {rnd+1}/{args.acq_rounds} [{model_type}-{acq_fn}] | MSE: {val_mse:.6f} | RMSE: {val_rmse:.6f}\")\n",
        "\n",
        "\n",
        "        key = f\"{acq_fn}-{model_type}\"\n",
        "\n",
        "\n",
        "        results_rmse[key] = rmse_curve\n",
        "        results_mse[key] = mse_curve\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "acff75cc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "acff75cc",
        "outputId": "eafc2518-e9d0-44ec-a81d-f0376a8438d9"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "friendly_labels = {\n",
        "    \"predictive-analytic\": \"Predictive (Analytic)\",\n",
        "    \"random-analytic\": \"Random (Analytic)\",\n",
        "    \"predictive-mfvi\": \"Predictive (MFVI)\",\n",
        "    \"random-mfvi\": \"Random (MFVI)\",\n",
        "}\n",
        "\n",
        "colors = {\n",
        "    \"analytic\": \"tab:blue\",\n",
        "    \"mfvi\": \"tab:red\",\n",
        "}\n",
        "\n",
        "linestyles = {\n",
        "    \"predictive\": \"-\",\n",
        "    \"random\": \"--\",\n",
        "}\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "if 'results_rmse' in globals():\n",
        "    for inf in models:\n",
        "        for acq in acq_fns:\n",
        "            key = f\"{acq}-{inf}\"\n",
        "            if key in results_rmse:\n",
        "                curve = results_rmse[key]\n",
        "                x_vals = np.arange(0, len(curve))\n",
        "                label = friendly_labels.get(key, key)\n",
        "                color = colors.get(inf, \"tab:gray\")\n",
        "                linestyle = linestyles.get(acq, \"-\")\n",
        "                plt.plot(x_vals, curve, label=label, linestyle=linestyle, color=color)\n",
        "plt.xlabel(\"Acquisition round\")\n",
        "plt.ylabel(\"RMSE\")\n",
        "plt.xlim(left=0,right=100)\n",
        "plt.title(\"Validation RMSE: Random vs Predictive (Analytic vs MFVI)\")\n",
        "plt.grid()\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
